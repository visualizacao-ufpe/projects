The term definition “Smart City” still allows various interpretations, and this causes some difficulty in establishing parameters to measure how smart the cities can be. This paper presents a Maturity Model that uses a set of minimum domains and indicators that aim to encourage cities of different sizes to identify their potential and improve processes and public policies. Public health; Big Data; Government; Smart Home. REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS

Since the rise of the Internet, the volume of metrics (indicators) used to measure current conditions of the service has been increasing exponentially. One of the challenges is to transform this amount data into legible information through visualization techniques. Visualization and monitoring indicators design is an actual and relevant challenge to provide intuitive, clear and direct understanding of large datasets. The solutions to the problem points to complex Graphical User Interface (GUI) that demands high cognitive efforts to handle it. This paper aims to present and evaluate the design process of a 3D visualization framework conceived to be easy to understand. We adopted user-centered design process to develop the concept model and usability concept to evaluate it. The usability evaluation was conducted using a Telehealth indicators dataset. Data were collected through observation technique and with the help of usability questionnaires. Data analysis was based on quantitative and qualitative approaches. The results describe the major advantages and limitations of the 3D visualization framework. We observed an overall good usability level. Meanwhile, users reported a total of 18 usability problems and proposed 35 suggestions to improve the user experience. They demonstrated strong motivation and interest in using the prototype. The usability problems of the visualization interface guided new improvements. Usability; Visualization; 3D REDES DE COMPUTADORES E SISTEMAS DISTRIBUIDOS MÍDIA E INTERAÇÃO

The severe effects of several types of noise present in tomographic image reconstruction are well known. Some of these are a consequence of the ill-posedness of this inverse problem. In this work, we investigate the impact of a Tikhonov regularization on the solution of a gamma-ray tomography reconstruction by means of a least squares numerical method. The theoretical methodology is considered in a broad sense as a Tikhonov regularization, but also includes the Morozov concept used specifically for the delta parameter control. The reconstruction quality shows effective improvement when this technique is applied to simple gamma-ray tomography algorithms. Furthermore, the impact of these regularization techniques on the solutions of linear systems of equations is significant. An ART (Algebraic Reconstruction Technique)-type algorithm was used for the reconstruction of simulated data utilizing built-in Matlab functions. These were compared with data obtained through a regularization implemented with TSVD (Truncated Singular Value Decomposition), as well as data obtained through hybrid algorithms such as TSVD plus Toepelitz, tridiagonal and identity operators. The quality of the resulting reconstruction is evaluated through RMSE (Root Mean Square Error). Direct comparisons suggest that for a high noise level and high delta parameter the TSVD plus tridiagonal operator is the best choice.

Software development is usually impacted by risks (e.g. schedule delays and increased costs), which may lead to the failure of the software project. Several techniques have been proposed to evaluate the effects of such undesirable issues, but probability estimates are usually neglected, affecting a proper evaluation of risks. This work presents an approach based on dependability models (e.g. stochastic Petri nets) for probabilistic evaluation of risks regarding the turnover of team members and requirement implementation in software development projects. Two case studies are adopted to demonstrate the feasibility of the proposed technique.

Hardware accelerators such as general-purpose GPUs and FPGAs have been used as an alternative to conventional CPU architectures in scientific computing applications, and have achieved good speed-up results. Within this context, the present study presents a heterogeneous architecture for high-performance computing based on CPUs and FPGAs, which efficiently explores the maximum parallelism degree for processing video segmentation using the concept of dynamic textures. The video segmentation algorithm includes processing the 3-D FFT, calculating the phase spectrum and the 2-D IFFT operation. The performance of the proposed architecture based on CPU and FPGA is compared with the reference implementation of FFTW in CPU and with the cuFFT library in GPU. The performance report of the prototyped architecture in a single Stratix IV FPGA obtained an overall speedup of 37x over the FFTW software library.

Several models have been presented to solve the financial time series forecasting problem. However, even with sophisticated techniques, a dilemma arises from all these models, called the random walk dilemma (RWD). In this context, the concept of time phase adjustment was proposed to overcome such dilemma for daily frequency financial time series. However, the evolution of trading platforms had increased the frequency for performing operations on the stock market to fractions of seconds, which makes needed the analysis of high-frequency financial time series. In this way, this work presents a model, called the increasing decreasing linear neuron (IDLN), for high-frequency stock market forecasting. Also, a descending gradient-based method with automatic time phase adjustment is presented for the proposed model design. Besides, an experimental analysis is conduced using a set of high-frequency financial time series from the Brazilian stock market, and the achieved results overcame those obtained by establishing forecasting models in the literature.

Test case (TC) selection is considered a hard problem, due to the high number of possible combinations to consider. Search-based optimization strategies arise as a promising way to treat this problem, as they explore the space of possible solutions (subsets of TCs), seeking the solution that best satisfies the given test adequacy criterion. The TC subsets are evaluated by an objective function, which must be optimized. In particular, we focus on multi-objective optimization (MOO) search-based strategies, which are able to properly treat TC selection problems with more than one test adequacy criterion.

In both academia and industry, there is a strong belief that multicore technology will radically change the way software is built. However, little is known about the current state of use of concurrent programming constructs. In this work we present an empirical work aimed at studying the usage of concurrent programming constructs of 2227 real world, stable and mature Java projects from SourceForge. We have studied the usage of concurrent techniques in the most recent versions of these applications and also how usage has evolved along time. The main findings of our study are: (I) More than 75% of the latest versions of the projects either explicitly create threads or employ some concurrency control mechanism. (II) More than half of these projects exhibit at least 47 synchronized methods and 3 implementations of the Runnable interface per 100,000 LoC, which means that not only concurrent programming constructs are used often but they are also employed intensively. (III) The adoption of the java.util.concurrent library is only moderate (approximately 23% of the concurrent projects employ it). (IV) Efficient and thread-safe data structures, such as ConcurrentHashMap, are not yet widely used, despite the fact that they present numerous advantages.

Recommender Systems (RSs) are software tools and techniques providing suggestions of relevant items to users. These systems have received increasing attention from both academy and industry since the 1990s, due to a variety of practical applications as well as complex problems to solve. Since then, the number of research papers published has increased significantly in many application domains (books, documents, images, movies, music, shopping, TV programs, and others). One of these domains has our attention in this paper due to the massive proliferation of televisions (TVs) with computational and network capabilities and due to the large amount of TV content and TV-related content available on the Web. With the evolution of TVs and RSs, the diversity of recommender systems for TV has increased substantially. In this direction, it is worth mentioning that we consider “recommender systems for TV” as those that make recommendations of both TV-content and any content related to TV. Due to this diversity, more investigation is necessary because research on recommender systems for TV domain is still broader and less mature than in other research areas. Thus, this literature review (LR) seeks to classify, synthesize, and present studies according to different perspectives of RSs in the television domain. For that, we initially identified, from the scientific literature, 282 relevant papers published from 2003 to May, 2015. The papers were then categorized and discussed according to different research and development perspectives: recommended item types, approaches, algorithms, architectural models, output devices, user profiling and evaluation. The obtained results can be useful to reveal trends and opportunities for both researchers and practitioners in the area.

This paper describes an innovative distributed framework for monitoring and control of large-scale systems by integrating heterogeneous smart objects, the world of physical devices, sensors and actuators, legacy devices and sub-systems, cooperating to support holistic management [1]. Its featured Service Oriented Architecture (SOA) exposes objects’ capabilities by means of web services, thus supporting syntactic and semantic interoperability among different technologies, including SCADA systems [23]. Wireless Sensor and Actuator Network (WSAN) devices and legacy subsystems cooperate while orchestrated by a manager in charge of enforcing a distributed logic. Particularly crafted for industrial networks are new middleware services such as dynamic spectrum management, distributed control logic, object virtualization, WSANs gateways, a SCADA gateway service, and data fusion transport capability. In addition, new application oriented objects such as shop floor, manufacturing line, welding station, robots, and cells have been introduced in the middleware. The combination of such objects and previous modules offers a new and flexible industry oriented middleware. A second contribution is in the form of traffic analysis conducted at the floor level. It shows the dominance of some end systems such as PLCs, the presence well behaved almost constant traffic made up of small packets.

This paper gives a multi-view relational fuzzy c-medoid vectors clustering algorithm that is able to partition objects taking into account simultaneously several dissimilarity matrices. The aim is to obtain a collaborative role of the different dissimilarity matrices in order to obtain a final consensus fuzzy partition. These matrices could have been obtained using different sets of variables and dissimilarity functions. This algorithm is designed to give a fuzzy partition and a vector of medoids for each fuzzy cluster as well as to learn a relevance weight for each dissimilarity matrix by optimizing an objective function. These relevance weights change at each iteration of the algorithm and are different from one cluster to another. Moreover, various tools for interpreting the fuzzy partition and fuzzy clusters provided by this algorithm are also presented. Several examples illustrate the performance and usefulness of the proposed algorithm.

Software-defined networking (SDN) has received a great deal of attention from both academia and industry in recent years. Studies on SDN have brought a number of interesting technical discussions on network architecture design, along with scientific contributions. Researchers, network operators, and vendors are trying to establish new standards and provide guidelines for proper implementation and deployment of such novel approach. It is clear that many of these research efforts have been made in the southbound of the SDN architecture, while the northbound interface still needs improvements. By focusing in the SDN northbound, this paper surveys the body of knowledge and discusses the challenges for developing SDN software. We investigate the existing solutions and identify trends and challenges on programming for SDN environments. We also discuss future developments on techniques, specifications, and methodologies for programmable networks, with the orthogonal view from the software engineering discipline.

The Requirements Traceability is seen as a quality factor with regard to software development, being present in standards and quality models. In this context, several techniques, models, frameworks and tools have been used to support it. Thus, the purpose of this paper is to present a systematic mapping carried out in order to find in the literature approaches to support the requirements traceability in the context of software projects and make the categorization of the data found in order to demonstrate, by means of a reliable, accurate and auditable method, how this area has developed and what are the main approaches are used to implement it. 

The protein kinase D isoenzymes PKD1/2/3 are prominent downstream targets of PKCs (Protein Kinase Cs) and phospholipase D in various biological systems. Recently, we identified PKD isoforms as novel mediators of tumour cell-endothelial cell communication, tumour cell motility and metastasis. Although PKD isoforms have been implicated in physiological/tumour angiogenesis, a role of PKDs during embryonic development, vasculogenesis and angiogenesis still remains elusive. We investigated the role of PKDs in germ layer segregation and subsequent vasculogenesis and angiogenesis using mouse embryonic stem cells (ESCs). We show that mouse ESCs predominantly express PKD2 followed by PKD3 while PKD1 displays negligible levels. Furthermore, we demonstrate that PKD2 is specifically phosphorylated/activated at the time of germ layer segregation. Time-restricted PKD2-activation limits mesendoderm formation and subsequent cardiovasculogenesis during early differentiation while leading to branching angiogenesis during late differentiation. In line, PKD2 loss-of-function analyses showed induction of mesendodermal differentiation in expense of the neuroectodermal germ layer. Our in vivo findings demonstrate that embryoid bodies transplanted on chicken chorioallantoic membrane induced an angiogenic response indicating that timed overexpression of PKD2 from day 4 onwards leads to augmented angiogenesis in differentiating ESCs. Taken together, our results describe novel and time-dependent facets of PKD2 during early cell fate determination.

According to the World Health Organization, breast cancer is the most common cancer in women worldwide, becoming one of the most fatal types of cancer. Mammography image analysis is still the most effective imaging technology for breast cancer diagnosis, which is based on texture and shape analysis of mammary lesions. The GrowCut algorithm is a general-purpose segmentation method based on cellular automata, able to perform relatively accurate segmentation through the adequate selection of internal and external seed points. In this work we propose an adaptive semi-supervised version of the GrowCut algorithm, based on the modification of the automaton evolution rule by adding a Gaussian fuzzy membership function in order to model non-defined borders. In our proposal, manual selection of seed points of the suspicious lesion is changed by a semiautomatic stage, where just the internal points are selected by using a differential evolution algorithm. We evaluated our proposal using 57 lesion images obtained from MiniMIAS database. Results were compared with the semi-supervised state-of-the-art approaches BEMD, BMCS, Wavelet Analysis, LBI, Topographic Approach and MCW. Results show that our method achieves better results for circumscribed, spiculated lesions and ill-defined lesions, considering the similarity between segmentation results and ground-truth images

In this paper, the authors aim to present the results of the literature review about the application of agile methods to support the implementation of CMMI (Capability Maturity Model Integration) and MPS.BR quality models, specifically for the technical solution process area and product design and construction process. This paper is to identify which agile methods are applied in the quality models context. In addition, they sought to identify agile practices that support the implementation of these processes.

This paper proposes a mapping between two product quality and software processes models used in the industry, the CERTICS national model and the CMMI-DEV international model. The stages of mapping are presented step by step, as well as the mapping review, which had the cooperation of one specialist in CERTICS and CMMI-DEV models. It aims to correlate the structures of the two models in order to facilitate and reduce the implementation time and costs, and to stimulate the execution of multi-model implementations in software developers companies.

The particulate matter (PM) concentration has been one of the most relevant environmental concerns in recent decades due to its prejudicial effects on living beings and the earth’s atmosphere. High PM concentration affects the human health in several ways leading to short and long term diseases. Thus, forecasting systems have been developed to support decisions of the organizations and governments to alert the population. Forecasting systems based on Artificial Neural Networks (ANNs) have been highlighted in the literature due to their performances. In general, three ANN-based approaches have been found for this task: ANN trained via learning algorithms, hybrid systems that combine search algorithms with ANNs, and hybrid systems that combine ANN with other forecasters. Independent of the approach, it is common to suppose that the residuals (error series), obtained from the difference between actual series and forecasting, have a white noise behavior. However, it is possible that this assumption is infringed due to: misspecification of the forecasting model, complexity of the time series or temporal patterns of the phenomenon not captured by the forecaster. This paper proposes an approach to improve the performance of PM forecasters from residuals modeling. The approach analyzes the remaining residuals recursively in search of temporal patterns. At each iteration, if there are temporal patterns in the residuals, the approach generates the forecasting of the residuals in order to improve the forecasting of the PM time series. The proposed approach can be used with either only one forecaster or by combining two or more forecasting models. In this study, the approach is used to improve the performance of a hybrid system (HS) composed by genetic algorithm (GA) and ANN from residuals modeling performed by two methods, namely, ANN and own hybrid system. Experiments were performed for PM2.5 and PM10 concentration series in Kallio and Vallila stations in Helsinki and evaluated from six metrics. Experimental results show that the proposed approach improves the accuracy of the forecasting method in terms of fitness function for all cases, when compared with the method without correction. The correction via HS obtained a superior performance, reaching the best results in terms of fitness function and in five out of six metrics. These results also were found when a sensitivity analysis was performed varying the proportions of the sets of training, validation and test. The proposed approach reached consistent results when compared with the forecasting method without correction, showing that it can be an interesting tool for correction of PM forecasters.

With the widespread adoption of social (aka Web 2.0) technologies like social networks, blogs, and wikis, there is a growing interest in looking into how enterprises should capitalize on these technologies. The social enterprise is the one that on top of having an online presence on the Internet, strives to open up new communication channels with stakeholders using social technologies. Building upon previous works on social business processes and on practical experiences on adopting such technologies in different Brazilian companies, this paper suggests an architecture and guiding framework for the social enterprise in terms of how to bridge the gap between the business world and social world and how to integrate social technologies into the enterprise's day-to-day operations.

Network virtualization has been pointed as a promising approach to solve Internet’s current ossification. A major challenge is the mapping of virtual networks (VNs) onto the substrate network due to its NP-hard nature, and, thus, several heuristics have been proposed aiming to achieve efficient allocations. However, the existing approaches only focus on performance, and dependability issues are usually neglected. Dependability involves metrics such as reliability and availability, which directly impact the quality of service, and a limitation is the absence of mechanisms for estimating dependability metrics in virtual networks. This paper proposes an automated approach for estimating dependability metrics in virtual network environments and a mapping algorithm based on GRASP metaheuristic. The approach is based on stochastic Petri nets (SPN) and reliability block diagrams (RBD), and a tool is also presented for automating model generation and evaluation. Experimental results demonstrate the feasibility of the proposed technique, as well as the trade-off between VN’s availability and cost.

Mobile learning (m-learning) is a research field that aims to analyze how mobile devices can contribute to learning. The development of software for mobile devices to support learning is essential for an effective implementation of m-learning or mobile learning environments (MLE). Requirements Engineering processes need to include activities that provoke creativity in the stakeholders to conceive MLEs that actually modify and improve the teaching and learning process. In this context, this paper presents a process for requirements elicitation and documentation of mobile learning environments. This process is based on the concepts of the Design Thinking process that provides a methodology to elicit customer needs, producing simple prototypes that eventually converge to innovative solutions. An experiment was conducted to evaluate if the proposed process contributes to create MLEs that present distinctive and interesting characteristics when compared to existing solutions for a specific problem. 

Most mainstream programming languages provide constructs to throw and to handle exceptions. However, several studies argue that exception handling code is usually of poor quality and that it is commonly neglected by developers. Moreover, it is said to be the least understood, documented, and tested part of the implementation of a system. Nevertheless, there are very few studies that analyze the actual exception handling bugs that occur in real software systems or that attempt to understand developers’ perceptions of these bugs. In this work we present an exploratory study on exception handling bugs that employs two complementary approaches: a survey of 154 developers and an analysis of 220 exception handling bugs from the repositories of Eclipse and Tomcat. Only 27% of the respondents claimed that policies and standards for the implementation of error handling are part of the culture of their organizations. Moreover, in 70% of the organizations there are no specific tests for the exception handling code. Also, 61% of the respondents stated that no to little importance is given to the documentation of exception handling in the design phase of the projects with which they are involved. In addition, about 40% of the respondents consider the quality of exception handling code to be either good or very good and only 14% of the respondents consider it to be bad or very bad. Furthermore, the repository analysis has shown (with statistical significance) that exception handling bugs are ignored by developers less often than other bugs. We have also observed that while overly general catch blocks are a well-known bad smell related to exceptions, bugs stemming from these catch blocks are rare, even though many overly general catch blocks occur in the code. Furthermore, while developers often mention empty catch blocks as causes of bugs they have fixed in the past, we found very few bug reports caused by them. On top of that, empty catch blocks are frequently used as part of bug fixes, including fixes for exception handling bugs. Based on our findings, we propose a classification of exception handling bugs and their causes. The proposed classification can be used to assist in the design and implementation of test suites, to guide code inspections, or as a basis for static analysis tools.

SysML is a variant of UML for systems design. Several formalisations of SysML (and UML) are available. Our work is distinctive in two ways: a semantics for refinement and for a representative collection of elements from the UML4SysML profile (blocks, state machines, activities, and interactions) used in combination. We provide a means to analyse and refine design models specified using SysML. This facilitates the discovery of problems earlier in the system development lifecycle, reducing time, and costs of production. Here, we describe our semantics, which is defined using a state-rich process algebra and implemented in a tool for automatic generation of formal models. We also show how the semantics can be used for refinement-based analysis and development. Our case study is a leadership-election protocol, a critical component of an industrial application. Our major contribution is a framework for reasoning using refinement about systems specified by collections of SysML diagrams.

In this paper, we present the design of an intelligent monitoring system consisting of physical sensors and intelligent software for the automatic identification of the concentration of natural gas odorants in the environment. An optical-based sensor array was proposed comprising the hardware module. The software module employs wavelets filters and artificial neural networks to recognize the concentration of odorant in a natural gas sample. The objective is to help the natural gas odorization process by means of end point monitoring through the recognizing of the odorant concentration. The recognizing process uses a benchmark index, which measures the degrees of human perception of gas in the environment. In this way, the proposed system tries to mimic the human perception of a natural gas leak and helps one to indicate if more or less amount of odorant should be added into the gas pipeline. Experiments were conducted comparing the performance of the system with human performance, which is normally used to deal with this problem. The proposed system demonstrated promising results and improvements are presented.

Nowadays, systems involving multiple FPGAs are used for various scientific applications. Such systems require a data bus dedicated to the communication between FPGAs, which could be done through a LVDS type. Another important factor is that the routing that interconnects the LVDS pins on the platform should be precisely developed to avoid instabilities in communication. Unfortunately, many platforms available in the market do not observe such restrictions, limiting the throughput of the bus.

There is an increase use of ontology-driven approaches to support requirements engineering (RE) activities, such as elicitation, analysis, specification, validation and management of requirements. However, the RE community still lacks a comprehensive understanding of how ontologies are used in RE process. Thus, the main objective of this work is to investigate and better understand how ontologies support RE as well as identify to what extent they have been applied to this field. In order to meet our goal, we conducted a systematic literature review (SLR) to identify the primary studies on the use of ontologies in RE, following a predefined review protocol. We then identified the main RE phases addressed, the requirements modelling styles that have been used in conjunction with ontologies, the types of requirements that have been supported by the use of ontologies and the ontology languages that have been adopted. We also examined the types of contributions reported and looked for evidences of the benefits of ontology-driven RE. In summary, the main findings of this work are: (1) there are empirical evidences of the benefits of using ontologies in RE activities both in industry and academy, specially for reducing ambiguity, inconsistency and incompleteness of requirements; (2) the majority of studies only partially address the RE process; (3) there is a great diversity of RE modelling styles supported by ontologies; (4) most studies addressed only functional requirements; (5) several studies describe the use/development of tools to support different types of ontology-driven RE approaches; (6) about half of the studies followed W3C recommendations on ontology-related languages; and (7) a great variety of RE ontologies were identified; nevertheless, none of them has been broadly adopted by the community. Finally, we conclude this work by showing several promising research opportunities that are quite important and interesting but underexplored in current research and practice.

In software product lines development, it is sometimes important to provide a flexible binding time for features such that developers can choose between static or dynamic feature activation. For example, software products designed for devices with constrained resources may use a static binding time to avoid the performance overhead introduced by dynamic binding time activation. However, other devices can exploit binding time flexibility to support products with a dynamic binding time for some of their features. To implement this kind of flexibility in a modular way, we can define AspectJ-based idioms. Researchers have proposed Edicts, an idiom based on AspectJ and design patterns. In this article, we argue that this idiom leads to an increase in code duplication, scattering, tangling and size, which can hamper code reuse, maintenance and understanding. To mitigate such issues, this paper proposes three idioms based on aspect-oriented programming to implement flexible feature binding. We apply our three idioms, along with Edicts, to implement a flexible binding time for features in four different applications. By doing so, we were able to assess the resulting implementations by using software metrics that judge code-quality factors. Our evaluation suggests that our idioms reduce the above-mentioned problems when implementing flexible feature binding for the selected features.

The use of business process standards to model and execute business needs is growing rapidly. In addition, Service-oriented Computing has been adopted to realize business processes, which basically consists of executing the process activities using services available in the Internet. In this context, the importance of security is apparent, because sensitive data sent over the Internet may be accessed by unauthorized third-parties. To prevent security problems, users may associate security requirements that must be enforced in essential tasks of the business process. This fact leads to the need of automation, because both functional and security requirements should be modeled, at high-level, and enforced, at execution level. This work proposes a cloud-based solution named BPA-Sec4Cloud that supports all phases of the security-aware business process automation, from its modeling to its deployment. The use of a cloud-based solution facilitates the deployment process because all needed resources are available in the cloud and ready to be used. In addition, the cloud is also used as a platform in order to provide specific services, such as translators, to support the automation process. In order to evaluate the BPA-Sec4Cloud, the solution was compared against existing solutions through the use of metrics related to the quality of generated artifacts.

Performance evaluation of mobile applications has received considerable attention as a prominent activity for improving services quality. Because many data stored on mobile device are synchronized with distributed data centers, the system availability is a critical attribute that requires investigation. Mobile backend-as-a-service (MBaaS) allows developers to link the backend of their applications to cloud storage, as well as providing device management and integration with social networking services. The OpenMobster platform offers a complete synchronization service for mobile applications, but its availability is an inherent critical issue, because one failure can result in losses for companies that use this environment. Analytical models can be used to assess availability of this type of environment and perhaps mitigate downtimes. This paper proposes a hierarchical model to assess the availability of the MBaaS OpenMobster platform focusing on two scenarios: the basic architecture and the automatic recovery process. The designed models were validated through testbed measurements by automatically injecting and repairing the infrastructure. Taking into account the three layers: hardware, operating system, and the MBaaS OpenMobster, we observed OpenMobster being the most critical service component. We have applied failover strategy on the Java virtual machine, and we obtained 10% of reduction in annual downtime. This work may guide systems' administrators in planning their maintenance policies.

Object-Oriented Programming is one of the most used paradigms. Complementarily, the software maintainability is considered a software attribute playing an important role in quality level. In this context, Object-Oriented Software Maintainability (OOSM) has been studied through years, and many researchers have proposed a large number of metrics to measure it. Consequently, the decision-making process about which metrics can be adopted in experiments on OOSM is a hard task. Therefore, a metrics’ categorization has been proposed to facilitate this process. As result, 7 categories and 17 subcategories were identified. These categories represent the scenarios of OOSM metrics adoption, and a family of OOSM metrics catalog was generated based on the selection of a metrics’ categorization. Additionally, a quasi-experiment was conducted to check the coverage index of the catalogs generated using our approach over the catalogs suggested by experts. 90% of coverage was obtained with 99% of confidential level using the Wilcoxon Test. Complementarily, a survey was conducted to check the experts’ opinion about the catalog generated by the portal when they were compared by the catalogs suggested by them. Therefore, this evaluation can be the first evidences of the usefulness of the family of the catalogs based on the metrics’ categorization.

Variant-rich software systems offer a large degree of customization, allowing users to configure the target system according to their preferences and needs. Facing high degrees of variability, these systems often employ variability models to explicitly capture user-configurable features (e.g., systems options) and the constraints they impose. The explicit representation of features allows them to be referenced in different variation points across different artifacts, enabling the latter to vary according to specific feature selections. In such settings, the evolution of variability models interplays with the evolution of related artifacts, requiring the two to evolve together, or coevolve. Interestingly, little is known about how such coevolution occurs in real-world systems, as existing research has focused mostly on variability evolution as it happens in variability models only. Furthermore, existing techniques supporting variability evolution are usually validated with randomly-generated variability models or evolution scenarios that do not stem from practice. As the community lacks a deep understanding of how variability evolution occurs in real-world systems and how it relates to the evolution of different kinds of software artifacts, it is not surprising that industry reports existing tools and solutions ineffective, as they do not handle the complexity found in practice. Attempting to mitigate this overall lack of knowledge and to support tool builders with insights on how variability models coevolve with other artifact types, we study a large and complex real-world variant-rich software system: the Linux kernel. Specifically, we extract variability-coevolution patterns capturing changes in the variability model of the Linux kernel with subsequent changes in Makefiles and C source code. From the analysis of the patterns, we report on findings concerning evolution principles found in the kernel, and we reveal deficiencies in existing tools and theory when handling changes captured by our patterns.

The abnormal deposition of amyloid-β protein in the brain plays an important role in Alzheimer's disease (AD), being considered a potential clinical biomarker. To investigate genetic associations with amyloid-β we used biomarker data and genome-wide variants from individuals with AD and mild cognitive impairment in the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. We used a standard linear model and retested the associations with a mixed linear model to correct the residual sample structure. Both methods' results showed two identical significant SNPs associated with the A β-42 levels in CSF (rs2075650 at intron region TOMM40 with p-value ≥ 1 × 10-16 and rs439401 in the intergenic region of LOC100129500 and APOC1 with p-value ≥ 1 × 10-9) and highlighted APOC1 and TOMM40, which are well-known genes previously associated with AD. Extending our analysis, we considered possible candidate genes mapped to SNPs with p-value ≥ 1 × 10-6 to explore gene-set enrichment e gene-gene network analysis, which reveals genes related to synaptic transmission, transmission of nerve impulses, cell-cell signaling and neurological processes. These genes require fine mapping and replication studies to allow more detailed understanding of how they may contribute to the genetic architecture of AD.

Bag-of-words is the most used representation method in text categorization. It represents each document as a feature vector where each vector position represents a word. Since all words in the database are considered features, the feature vector can reach tens of thousands of features. Therefore, text categorization relies on feature selection to eliminate meaningless data and to reduce the execution time. In this paper, we propose two filtering methods for feature selection in text categorization, namely: Maximum f Features per Document (MFD), and Maximum f Features per Document – Reduced (MFDR). Both algorithms determine the number of selected features f in a data-driven way using a global-ranking Feature Evaluation Function (FEF). The MFD method analyzes all documents to ensure that each document in the training set is represented in the final feature vector. Whereas MFDR analyzes only the documents with high FEF valued features to select less features therefore avoiding unnecessary ones. The experimental study evaluated the effectiveness of the proposed methods on four text categorization databases (20 Newsgroup, Reuters, WebKB and TDT2) and three FEFs using the Naïve Bayes classifier. The proposed methods present better or equivalent results when compared with the ALOFT method, in all cases, and Variable Ranking, in more than 93% of the cases.

Efficient techniques for pattern matching are essential in a number of networked systems and services, such as intrusion detection systems, application identification and classification services, and traffic management. Most pattern matching applications describe patterns using regular expression and the support engine is Deterministic Finite Automaton (FA). Previous research studies address either performance or space requirements issues. From the original DFA formalism we design and evaluate optimizations to its representation and operation to meet Deep Packet Inspection (DPI) systems’ requirements for commodity platforms, such as (i) decreasing the original DFA memory consumption (high compression ratio) and (ii) performing pattern matching as fast as the original DFA. Our approach spans from designing the DFA to developing memory layouts to get the most of the underlying architecture. The contributions of this work are threefold: (i) a new and improved finite automaton model, called Ranged Compressed DFA (RCDFA), (ii) three RCDFA optimizations for achieving more compression and matching speed, and (iii) three advanced layouts for implementing the compressed automaton without memory and performance penalties. The experimental evaluation shows that RCDFA compresses DFA up to 99% without imposing additional memory lookups. The proposed advanced layouts reach memory compression of around 97%. RCDFA together with the advanced layouts outperforms the standard DFA by up to 32 times in terms of processing speed.

It appears uncontroversial that digital consumers deserve the same level of protection for their transactions as that offered in other forms of commerce. The question is whether current consumer protection instruments provide these protections. Courts in Europe have adopted a view that, by analogy, one should grant 'product-protection' to digital content contracts on a case-by-case basis. Unfortunately, this means that the digital content category remains highly controversial and uncertain to the stakeholders of such transactions. On the other hand, the Brazilian Consumer Protection Law (CDC) has adopted more innovative approaches to consumer rights than many European Union (EU) Member States' national legislation, both when it has a broad understanding of 'goods' and 'services' and when it provides just minor differences between the legal remedies and rights for the sale of goods and service contracts. Here, we argue that there is no reason to craft a third sui generis category for digital content per se, and to the cloud-based transactions, for the supply of digital content in Brazil, if the current legal framework in the Brazilian CDC in relation to these issues is interpreted extensively. We further claim that this remains the best approach to be adopted.

This article proposes a new data collision free medium access control protocol for wireless sensor networks in automation environments based on Time Division Multiple Access (TDMA) with duty cycling, which presents less energy consumption than the S-MAC protocol while maintain high data throughput. The new protocol employs single channel and a carrier sense approach. Simulation results are presented and show how the new proposal outperforms the S-MAC protocol.

Modularity benefits, including the independent maintenance and comprehension of individual modules, have been widely advocated. However, empirical assessments to investigate those benefits have mostly focused on source code, and thus, the relevance of modularity to earlier artifacts is still not so clear (such as requirements and design models). In this paper, we use a multimethod technique, including designed experiments, to empirically evaluate the benefits of modularity in the context of two approaches for specifying product line use case scenarios: PLUSS and MSVCM. The first uses an annotative approach for specifying variability, whereas the second relies on aspect-oriented constructs for separating common and variant scenario specifications. After evaluating these approaches through the specifications of several systems, we find out that MSVCM reduces feature scattering and improves scenario cohesion. These results suggest that evolving a product line specification using MSVCM requires only localized changes. On the other hand, the results of six experiments reveal that MSVCM requires more time to derive the product line specifications and, contrasting with the modularity results, reduces the time to evolve a product line specification only when the subjects have been well trained and are used to the task of evolving product line specifications.
