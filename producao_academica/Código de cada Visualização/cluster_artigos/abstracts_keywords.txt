The term definition "Smart City" still allows various interpretations, and this causes some difficulty in establishing parameters to measure how smart the cities can be. This paper presents a Maturity Model that uses a set of minimum domains and indicators that aim to encourage cities of different sizes to identify their potential and improve processes and public policies.	
Since the rise of the Internet, the volume of metrics (indicators) used to measure current conditions of the service has been increasing exponentially. One of the challenges is to transform this amount data into legible information through visualization techniques. Visualization and monitoring indicators design is an actual and relevant challenge to provide intuitive, clear and direct understanding of large datasets. The solutions to the problem points to complex Graphical User Interface (GUI) that demands high cognitive efforts to handle it. This paper aims to present and evaluate the design process of a 3D visualization framework conceived to be easy to understand. We adopted user-centered design process to develop the concept model and usability concept to evaluate it. The usability evaluation was conducted using a Telehealth indicators dataset. Data were collected through observation technique and with the help of usability questionnaires. Data analysis was based on quantitative and qualitative approaches. The results describe the major advantages and limitations of the 3D visualization framework. We observed an overall good usability level. Meanwhile, users reported a total of 18 usability problems and proposed 35 suggestions to improve the user experience. They demonstrated strong motivation and interest in using the prototype. The usability problems of the visualization interface guided new improvements.	Usability; Visualization; 3D
The severe effects of several types of noise present in tomographic image reconstruction are well known. Some of these are a consequence of the ill-posedness of this inverse problem. In this work, we investigate the impact of a Tikhonov regularization on the solution of a gamma-ray tomography reconstruction by means of a least squares numerical method. The theoretical methodology is considered in a broad sense as a Tikhonov regularization, but also includes the Morozov concept used specifically for the delta parameter control. The reconstruction quality shows effective improvement when this technique is applied to simple gamma-ray tomography algorithms. Furthermore, the impact of these regularization techniques on the solutions of linear systems of equations is significant. An ART (Algebraic Reconstruction Technique)-type algorithm was used for the reconstruction of simulated data utilizing built-in Matlab functions. These were compared with data obtained through a regularization implemented with TSVD (Truncated Singular Value Decomposition), as well as data obtained through hybrid algorithms such as TSVD plus Toepelitz, tridiagonal and identity operators. The quality of the resulting reconstruction is evaluated through RMSE (Root Mean Square Error). Direct comparisons suggest that for a high noise level and high delta parameter the TSVD plus tridiagonal operator is the best choice.	Regularization operators; SVD optimization; Inverse problem; Gamma transmission
Software development is usually impacted by risks (e.g. schedule delays and increased costs), which may lead to the failure of the software project. Several techniques have been proposed to evaluate the effects of such undesirable issues, but probability estimates are usually neglected, affecting a proper evaluation of risks. This work presents an approach based on dependability models (e.g. stochastic Petri nets) for probabilistic evaluation of risks regarding the turnover of team members and requirement implementation in software development projects. Two case studies are adopted to demonstrate the feasibility of the proposed technique.	software evaluation, software development risk, probability estimates, dependability models, probabilistic evaluation, software development project
Hardware accelerators such as general-purpose GPUs and FPGAs have been used as an alternative to conventional CPU architectures in scientific computing applications, and have achieved good speed-up results. Within this context, the present study presents a heterogeneous architecture for high-performance computing based on CPUs and FPGAs, which efficiently explores the maximum parallelism degree for processing video segmentation using the concept of dynamic textures. The video segmentation algorithm includes processing the 3-D FFT, calculating the phase spectrum and the 2-D IFFT operation. The performance of the proposed architecture based on CPU and FPGA is compared with the reference implementation of FFTW in CPU and with the cuFFT library in GPU. The performance report of the prototyped architecture in a single Stratix IV FPGA obtained an overall speedup of 37x over the FFTW software library.	Video segmentation; FFT; IFFT; FPGA; Hardware architecture; High performance system
Several models have been presented to solve the financial time series forecasting problem. However, even with sophisticated techniques, a dilemma arises from all these models, called the random walk dilemma (RWD). In this context, the concept of time phase adjustment was proposed to overcome such dilemma for daily frequency financial time series. However, the evolution of trading platforms had increased the frequency for performing operations on the stock market to fractions of seconds, which makes needed the analysis of high-frequency financial time series. In this way, this work presents a model, called the increasing decreasing linear neuron (IDLN), for high-frequency stock market forecasting. Also, a descending gradient-based method with automatic time phase adjustment is presented for the proposed model design. Besides, an experimental analysis is conduced using a set of high-frequency financial time series from the Brazilian stock market, and the achieved results overcame those obtained by establishing forecasting models in the literature.	Artificial neuron; Descending gradient-based learning; Forecasting; High-frequency stock market
Test case (TC) selection is considered a hard problem, due to the high number of possible combinations to consider. Search-based optimization strategies arise as a promising way to treat this problem, as they explore the space of possible solutions (subsets of TCs), seeking the solution that best satisfies the given test adequacy criterion. The TC subsets are evaluated by an objective function, which must be optimized. In particular, we focus on multi-objective optimization (MOO) search-based strategies, which are able to properly treat TC selection problems with more than one test adequacy criterion.	Multi-objective test case selectionSoftware testingParticle swarm optimizationHarmony searchMulti-objective optimization
In both academia and industry, there is a strong belief that multicore technology will radically change the way software is built. However, little is known about the current state of use of concurrent programming constructs. In this work we present an empirical work aimed at studying the usage of concurrent programming constructs of 2227 real world, stable and mature Java projects from SourceForge. We have studied the usage of concurrent techniques in the most recent versions of these applications and also how usage has evolved along time. The main findings of our study are: (I) More than 75% of the latest versions of the projects either explicitly create threads or employ some concurrency control mechanism. (II) More than half of these projects exhibit at least 47 synchronized methods and 3 implementations of the Runnable interface per 100,000 LoC, which means that not only concurrent programming constructs are used often but they are also employed intensively. (III) The adoption of the java.util.concurrent library is only moderate (approximately 23% of the concurrent projects employ it). (IV) Efficient and thread-safe data structures, such as ConcurrentHashMap, are not yet widely used, despite the fact that they present numerous advantages.	Java; Concurrency; Software evolution
Recommender Systems (RSs) are software tools and techniques providing suggestions of relevant items to users. These systems have received increasing attention from both academy and industry since the 1990s, due to a variety of practical applications as well as complex problems to solve. Since then, the number of research papers published has increased significantly in many application domains (books, documents, images, movies, music, shopping, TV programs, and others). One of these domains has our attention in this paper due to the massive proliferation of televisions (TVs) with computational and network capabilities and due to the large amount of TV content and TV-related content available on the Web. With the evolution of TVs and RSs, the diversity of recommender systems for TV has increased substantially. In this direction, it is worth mentioning that we consider “recommender systems for TV” as those that make recommendations of both TV-content and any content related to TV. Due to this diversity, more investigation is necessary because research on recommender systems for TV domain is still broader and less mature than in other research areas. Thus, this literature review (LR) seeks to classify, synthesize, and present studies according to different perspectives of RSs in the television domain. For that, we initially identified, from the scientific literature, 282 relevant papers published from 2003 to May, 2015. The papers were then categorized and discussed according to different research and development perspectives: recommended item types, approaches, algorithms, architectural models, output devices, user profiling and evaluation. The obtained results can be useful to reveal trends and opportunities for both researchers and practitioners in the area.	Literature review; Recommender systems; TV-content; TV-related content
This paper describes an innovative distributed framework for monitoring and control of large-scale systems by integrating heterogeneous smart objects, the world of physical devices, sensors and actuators, legacy devices and sub-systems, cooperating to support holistic management [1]. Its featured Service Oriented Architecture (SOA) exposes objects’ capabilities by means of web services, thus supporting syntactic and semantic interoperability among different technologies, including SCADA systems [23]. Wireless Sensor and Actuator Network (WSAN) devices and legacy subsystems cooperate while orchestrated by a manager in charge of enforcing a distributed logic. Particularly crafted for industrial networks are new middleware services such as dynamic spectrum management, distributed control logic, object virtualization, WSANs gateways, a SCADA gateway service, and data fusion transport capability. In addition, new application oriented objects such as shop floor, manufacturing line, welding station, robots, and cells have been introduced in the middleware. The combination of such objects and previous modules offers a new and flexible industry oriented middleware. A second contribution is in the form of traffic analysis conducted at the floor level. It shows the dominance of some end systems such as PLCs, the presence well behaved almost constant traffic made up of small packets.	Computer networks; Middleware; Industrial networks; Wireless sensor networks
This paper gives a multi-view relational fuzzy c-medoid vectors clustering algorithm that is able to partition objects taking into account simultaneously several dissimilarity matrices. The aim is to obtain a collaborative role of the different dissimilarity matrices in order to obtain a final consensus fuzzy partition. These matrices could have been obtained using different sets of variables and dissimilarity functions. This algorithm is designed to give a fuzzy partition and a vector of medoids for each fuzzy cluster as well as to learn a relevance weight for each dissimilarity matrix by optimizing an objective function. These relevance weights change at each iteration of the algorithm and are different from one cluster to another. Moreover, various tools for interpreting the fuzzy partition and fuzzy clusters provided by this algorithm are also presented. Several examples illustrate the performance and usefulness of the proposed algorithm.	
Service-Oriented Architecture (SOA) is a paradigm for software development based on the concept of service. In SOA, the Quality of Services (QoS) impacts on the status of a business and on the relationship between service customers and providers. As customers expect to receive services with quality no less than they have paid for, it is usual to stress a SOA application in order to measure its QoS levels, which can be expensive and time consuming. This paper shows that the behavior of a SOA system can be modeled by Petri Nets and, from the model, QoS levels (performance and availability) can be estimated. In this way, the analysis can be conducted without necessarily implementing the real system, which tends to be valuable in the design phase of SOA. Additionally, we present a methodology to implement the model findings, which allows to verify its accuracy in practice. As a final contribution we associate our modeling approach to the Service Level Agreements (SLA) composition, which allows to discover and prevent bottlenecks delaying the system and to anticipate potential SLA violations. Two examples illustrate our results.	Petri Nets; Service-Oriented Architectures; Service orchestration; Modeling; Simulation
Software-defined networking (SDN) has received a great deal of attention from both academia and industry in recent years. Studies on SDN have brought a number of interesting technical discussions on network architecture design, along with scientific contributions. Researchers, network operators, and vendors are trying to establish new standards and provide guidelines for proper implementation and deployment of such novel approach. It is clear that many of these research efforts have been made in the southbound of the SDN architecture, while the northbound interface still needs improvements. By focusing in the SDN northbound, this paper surveys the body of knowledge and discusses the challenges for developing SDN software. We investigate the existing solutions and identify trends and challenges on programming for SDN environments. We also discuss future developments on techniques, specifications, and methodologies for programmable networks, with the orthogonal view from the software engineering discipline.	software engineering discipline, software engineering perspective, SDN programmability, software defined networking, network architecture design, northbound interface, SDN environments, programmable networks
The Requirements Traceability is seen as a quality factor with regard to software development, being present in standards and quality models. In this context, several techniques, models, frameworks and tools have been used to support it. Thus, the purpose of this paper is to present a systematic mapping carried out in order to find in the literature approaches to support the requirements traceability in the context of software projects and make the categorization of the data found in order to demonstrate, by means of a reliable, accurate and auditable method, how this area has developed and what are the main approaches are used to implement it. 	Systematic Mapping, Requirements Traceability, Requirements Management.
The protein kinase D isoenzymes PKD1/2/3 are prominent downstream targets of PKCs (Protein Kinase Cs) and phospholipase D in various biological systems. Recently, we identified PKD isoforms as novel mediators of tumour cell-endothelial cell communication, tumour cell motility and metastasis. Although PKD isoforms have been implicated in physiological/tumour angiogenesis, a role of PKDs during embryonic development, vasculogenesis and angiogenesis still remains elusive. We investigated the role of PKDs in germ layer segregation and subsequent vasculogenesis and angiogenesis using mouse embryonic stem cells (ESCs). We show that mouse ESCs predominantly express PKD2 followed by PKD3 while PKD1 displays negligible levels. Furthermore, we demonstrate that PKD2 is specifically phosphorylated/activated at the time of germ layer segregation. Time-restricted PKD2-activation limits mesendoderm formation and subsequent cardiovasculogenesis during early differentiation while leading to branching angiogenesis during late differentiation. In line, PKD2 loss-of-function analyses showed induction of mesendodermal differentiation in expense of the neuroectodermal germ layer. Our in vivo findings demonstrate that embryoid bodies transplanted on chicken chorioallantoic membrane induced an angiogenic response indicating that timed overexpression of PKD2 from day 4 onwards leads to augmented angiogenesis in differentiating ESCs. Taken together, our results describe novel and time-dependent facets of PKD2 during early cell fate determination.	
According to the World Health Organization, breast cancer is the most common cancer in women worldwide, becoming one of the most fatal types of cancer. Mammography image analysis is still the most effective imaging technology for breast cancer diagnosis, which is based on texture and shape analysis of mammary lesions. The GrowCut algorithm is a general-purpose segmentation method based on cellular automata, able to perform relatively accurate segmentation through the adequate selection of internal and external seed points. In this work we propose an adaptive semi-supervised version of the GrowCut algorithm, based on the modification of the automaton evolution rule by adding a Gaussian fuzzy membership function in order to model non-defined borders. In our proposal, manual selection of seed points of the suspicious lesion is changed by a semiautomatic stage, where just the internal points are selected by using a differential evolution algorithm. We evaluated our proposal using 57 lesion images obtained from MiniMIAS database. Results were compared with the semi-supervised state-of-the-art approaches BEMD, BMCS, Wavelet Analysis, LBI, Topographic Approach and MCW. Results show that our method achieves better results for circumscribed, spiculated lesions and ill-defined lesions, considering the similarity between segmentation results and ground-truth images.	Breast cancer; Mass detection; Mammography image analysis; Biomedical image segmentation; GrowCut algorithm
This paper aims to present the results of the literature review about the application of agile methods to support the implementation of CMMI and MPS.BR quality models, specifically for the Technical Solution process area and Product Design and Construction process. The research result is to identify which agile methods are applied in the quality models context. In addition, we sought to identify agile practices that support the implementation of these processes. 	Agile Methods, Software Quality Models, Technical Solution, Product Design and Construction, Literature Review. 
This paper proposes a mapping between two product quality and software processes models used in the industry, the CERTICS national model and the CMMI-DEV international model. The stages of mapping are presented step by step, as well as the mapping review, which had the cooperation of one specialist in CERTICS and CMMI-DEV models. It aims to correlate the structures of the two models in order to facilitate and reduce the implementation time and costs, and to stimulate the execution of multi-model implementations in software developers companies.	CERTICS, CMMI-DEV, Model Mapping, Multi-Models Quality Models, Software Quality.
The particulate matter (PM) concentration has been one of the most relevant environmental concerns in recent decades due to its prejudicial effects on living beings and the earth’s atmosphere. High PM concentration affects the human health in several ways leading to short and long term diseases. Thus, forecasting systems have been developed to support decisions of the organizations and governments to alert the population. Forecasting systems based on Artificial Neural Networks (ANNs) have been highlighted in the literature due to their performances. In general, three ANN-based approaches have been found for this task: ANN trained via learning algorithms, hybrid systems that combine search algorithms with ANNs, and hybrid systems that combine ANN with other forecasters. Independent of the approach, it is common to suppose that the residuals (error series), obtained from the difference between actual series and forecasting, have a white noise behavior. However, it is possible that this assumption is infringed due to: misspecification of the forecasting model, complexity of the time series or temporal patterns of the phenomenon not captured by the forecaster. This paper proposes an approach to improve the performance of PM forecasters from residuals modeling. The approach analyzes the remaining residuals recursively in search of temporal patterns. At each iteration, if there are temporal patterns in the residuals, the approach generates the forecasting of the residuals in order to improve the forecasting of the PM time series. The proposed approach can be used with either only one forecaster or by combining two or more forecasting models. In this study, the approach is used to improve the performance of a hybrid system (HS) composed by genetic algorithm (GA) and ANN from residuals modeling performed by two methods, namely, ANN and own hybrid system. Experiments were performed for PM2.5and PM10 concentration series in Kallio and Vallila stations in Helsinki and evaluated from six metrics. Experimental results show that the proposed approach improves the accuracy of the forecasting method in terms of fitness function for all cases, when compared with the method without correction. The correction via HS obtained a superior performance, reaching the best results in terms of fitness function and in five out of six metrics. These results also were found when a sensitivity analysis was performed varying the proportions of the sets of training, validation and test. The proposed approach reached consistent results when compared with the forecasting method without correction, showing that it can be an interesting tool for correction of PM forecasters.	
With the widespread adoption of social (aka Web 2.0) technologies like social networks, blogs, and wikis, there is a growing interest in looking into how enterprises should capitalize on these technologies. The social enterprise is the one that on top of having an online presence on the Internet, strives to open up new communication channels with stakeholders using social technologies. Building upon previous works on social business processes and on practical experiences on adopting such technologies in different Brazilian companies, this paper suggests an architecture and guiding framework for the social enterprise in terms of how to bridge the gap between the business world and social world and how to integrate social technologies into the enterprise's day-to-day operations.	enterprise architecture, guiding framework, social enterprise, social technologies, social networks, blogs, wikis, communication channels, enterprise operations
Network virtualization has been pointed as a promising approach to solve Internet’s current ossification. A major challenge is the mapping of virtual networks (VNs) onto the substrate network due to its NP-hard nature, and, thus, several heuristics have been proposed aiming to achieve efficient allocations. However, the existing approaches only focus on performance, and dependability issues are usually neglected. Dependability involves metrics such as reliability and availability, which directly impact the quality of service, and a limitation is the absence of mechanisms for estimating dependability metrics in virtual networks. This paper proposes an automated approach for estimating dependability metrics in virtual network environments and a mapping algorithm based on GRASP metaheuristic. The approach is based on stochastic Petri nets (SPN) and reliability block diagrams (RBD), and a tool is also presented for automating model generation and evaluation. Experimental results demonstrate the feasibility of the proposed technique, as well as the trade-off between VN’s availability and cost.	Network Virtualization; Dependability; Stochastic Petri nets; Reliability block diagrams
Mobile learning (m-learning) is a research field that aims to analyze how mobile devices can contribute to learning. The development of software for mobile devices to support learning is essential for an effective implementation of m-learning or mobile learning environments (MLE). Requirements Engineering processes need to include activities that provoke creativity in the stakeholders to conceive MLEs that actually modify and improve the teaching and learning process. In this context, this paper presents a process for requirements elicitation and documentation of mobile learning environments. This process is based on the concepts of the Design Thinking process that provides a methodology to elicit customer needs, producing simple prototypes that eventually converge to innovative solutions. An experiment was conducted to evaluate if the proposed process contributes to create MLEs that present distinctive and interesting characteristics when compared to existing solutions for a specific problem.	Mobile Learning, Mobile Learning Environments, Design Thinking, Requirements Engineering.
Most mainstream programming languages provide constructs to throw and to handle exceptions. However, several studies argue that exception handling code is usually of poor quality and that it is commonly neglected by developers. Moreover, it is said to be the least understood, documented, and tested part of the implementation of a system. Nevertheless, there are very few studies that analyze the actual exception handling bugs that occur in real software systems or that attempt to understand developers’ perceptions of these bugs. In this work we present an exploratory study on exception handling bugs that employs two complementary approaches: a survey of 154 developers and an analysis of 220 exception handling bugs from the repositories of Eclipse and Tomcat. Only 27% of the respondents claimed that policies and standards for the implementation of error handling are part of the culture of their organizations. Moreover, in 70% of the organizations there are no specific tests for the exception handling code. Also, 61% of the respondents stated that no to little importance is given to the documentation of exception handling in the design phase of the projects with which they are involved. In addition, about 40% of the respondents consider the quality of exception handling code to be either good or very good and only 14% of the respondents consider it to be bad or very bad. Furthermore, the repository analysis has shown (with statistical significance) that exception handling bugs are ignored by developers less often than other bugs. We have also observed that while overly general catch blocks are a well-known bad smell related to exceptions, bugs stemming from these catch blocks are rare, even though many overly general catch blocks occur in the code. Furthermore, while developers often mention empty catch blocks as causes of bugs they have fixed in the past, we found very few bug reports caused by them. On top of that, empty catch blocks are frequently used as part of bug fixes, including fixes for exception handling bugs. Based on our findings, we propose a classification of exception handling bugs and their causes. The proposed classification can be used to assist in the design and implementation of test suites, to guide code inspections, or as a basis for static analysis tools.	
SysML is a variant of UML for systems design. Several formalisations of SysML (and UML) are available. Our work is distinctive in two ways: a semantics for refinement and for a representative collection of elements from the UML4SysML profile (blocks, state machines, activities, and interactions) used in combination. We provide a means to analyse and refine design models specified using SysML. This facilitates the discovery of problems earlier in the system development lifecycle, reducing time, and costs of production. Here, we describe our semantics, which is defined using a state-rich process algebra and implemented in a tool for automatic generation of formal models. We also show how the semantics can be used for refinement-based analysis and development. Our case study is a leadership-election protocol, a critical component of an industrial application. Our major contribution is a framework for reasoning using refinement about systems specified by collections of SysML diagrams.	Exception handling; Bugs; Repository mining
In this paper, we present the design of an intelligent monitoring system consisting of physical sensors and intelligent software for the automatic identification of the concentration of natural gas odorants in the environment. An optical-based sensor array was proposed comprising the hardware module. The software module employs wavelets filters and artificial neural networks to recognize the concentration of odorant in a natural gas sample. The objective is to help the natural gas odorization process by means of end point monitoring through the recognizing of the odorant concentration. The recognizing process uses a benchmark index, which measures the degrees of human perception of gas in the environment. In this way, the proposed system tries to mimic the human perception of a natural gas leak and helps one to indicate if more or less amount of odorant should be added into the gas pipeline. Experiments were conducted comparing the performance of the system with human performance, which is normally used to deal with this problem. The proposed system demonstrated promising results and improvements are presented.	wavelet neural nets, chemical variables measurement, computerised monitoring, electronic noses, filtration, intelligent sensors, natural gas technology, optical sensors, pipelines, sensor arrays
Nowadays, systems involving multiple FPGAs are used for various scientific applications. Such systems require a data bus dedicated to the communication between FPGAs, which could be done through a LVDS type. Another important factor is that the routing that interconnects the LVDS pins on the platform should be precisely developed to avoid instabilities in communication. Unfortunately, many platforms available in the market do not observe such restrictions, limiting the throughput of the bus.	LVDS FPGA CRC Communication inter-FPGAs
There is an increase use of ontology-driven approaches to support requirements engineering (RE) activities, such as elicitation, analysis, specification, validation and management of requirements. However, the RE community still lacks a comprehensive understanding of how ontologies are used in RE process. Thus, the main objective of this work is to investigate and better understand how ontologies support RE as well as identify to what extent they have been applied to this field. In order to meet our goal, we conducted a systematic literature review (SLR) to identify the primary studies on the use of ontologies in RE, following a predefined review protocol. We then identified the main RE phases addressed, the requirements modelling styles that have been used in conjunction with ontologies, the types of requirements that have been supported by the use of ontologies and the ontology languages that have been adopted. We also examined the types of contributions reported and looked for evidences of the benefits of ontology-driven RE. In summary, the main findings of this work are: (1) there are empirical evidences of the benefits of using ontologies in RE activities both in industry and academy, specially for reducing ambiguity, inconsistency and incompleteness of requirements; (2) the majority of studies only partially address the RE process; (3) there is a great diversity of RE modelling styles supported by ontologies; (4) most studies addressed only functional requirements; (5) several studies describe the use/development of tools to support different types of ontology-driven RE approaches; (6) about half of the studies followed W3C recommendations on ontology-related languages; and (7) a great variety of RE ontologies were identified; nevertheless, none of them has been broadly adopted by the community. Finally, we conclude this work by showing several promising research opportunities that are quite important and interesting but underexplored in current research and practice.	Ontologies Requirements engineering Systematic literature review
In software product lines development, it is sometimes important to provide a flexible binding time for features such that developers can choose between static or dynamic feature activation. For example, software products designed for devices with constrained resources may use a static binding time to avoid the performance overhead introduced by dynamic binding time activation. However, other devices can exploit binding time flexibility to support products with a dynamic binding time for some of their features. To implement this kind of flexibility in a modular way, we can define AspectJ-based idioms. Researchers have proposed Edicts, an idiom based on AspectJ and design patterns. In this article, we argue that this idiom leads to an increase in code duplication, scattering, tangling and size, which can hamper code reuse, maintenance and understanding. To mitigate such issues, this paper proposes three idioms based on aspect-oriented programming to implement flexible feature binding. We apply our three idioms, along with Edicts, to implement a flexible binding time for features in four different applications. By doing so, we were able to assess the resulting implementations by using software metrics that judge code-quality factors. Our evaluation suggests that our idioms reduce the above-mentioned problems when implementing flexible feature binding for the selected features.	flexible binding time software product lines software metrics aspect-oriented programming
The use of business process standards to model and execute business needs is growing rapidly. In addition, Service-oriented Computing has been adopted to realize business processes, which basically consists of executing the process activities using services available in the Internet. In this context, the importance of security is apparent, because sensitive data sent over the Internet may be accessed by unauthorized third-parties. To prevent security problems, users may associate security requirements that must be enforced in essential tasks of the business process. This fact leads to the need of automation, because both functional and security requirements should be modeled, at high-level, and enforced, at execution level. This work proposes a cloud-based solution named BPA-Sec4Cloud that supports all phases of the security-aware business process automation, from its modeling to its deployment. The use of a cloud-based solution facilitates the deployment process because all needed resources are available in the cloud and ready to be used. In addition, the cloud is also used as a platform in order to provide specific services, such as translators, to support the automation process. In order to evaluate the BPA-Sec4Cloud, the solution was compared against existing solutions through the use of metrics related to the quality of generated artifacts.	68M14 Distributed systems68M11 Internet topics68U35 Information systems
Performance evaluation of mobile applications has received considerable attention as a prominent activity for improving services quality. Because many data stored on mobile device are synchronized with distributed data centers, the system availability is a critical attribute that requires investigation. Mobile backend-as-a-service (MBaaS) allows developers to link the backend of their applications to cloud storage, as well as providing device management and integration with social networking services. The OpenMobster platform offers a complete synchronization service for mobile applications, but its availability is an inherent critical issue, because one failure can result in losses for companies that use this environment. Analytical models can be used to assess availability of this type of environment and perhaps mitigate downtimes. This paper proposes a hierarchical model to assess the availability of the MBaaS OpenMobster platform focusing on two scenarios: the basic architecture and the automatic recovery process. The designed models were validated through testbed measurements by automatically injecting and repairing the infrastructure. Taking into account the three layers: hardware, operating system, and the MBaaS OpenMobster, we observed OpenMobster being the most critical service component. We have applied failover strategy on the Java virtual machine, and we obtained 10% of reduction in annual downtime. This work may guide systems' administrators in planning their maintenance policies.	
Object-Oriented Programming is one of the most used paradigms. Complementarily, the software maintainability is considered a software attribute playing an important role in quality level. In this context, Object-Oriented Software Maintainability (OOSM) has been studied through years, and many researchers have proposed a large number of metrics to measure it. Consequently, the decision-making process about which metrics can be adopted in experiments on OOSM is a hard task. Therefore, a metrics’ categorization has been proposed to facilitate this process. As result, 7 categories and 17 subcategories were identified. These categories represent the scenarios of OOSM metrics adoption, and a family of OOSM metrics catalog was generated based on the selection of a metrics’ categorization. Additionally, a quasi-experiment was conducted to check the coverage index of the catalogs generated using our approach over the catalogs suggested by experts. 90% of coverage was obtained with 99% of confidential level using the Wilcoxon Test. Complementarily, a survey was conducted to check the experts’ opinion about the catalog generated by the portal when they were compared by the catalogs suggested by them. Therefore, this evaluation can be the first evidences of the usefulness of the family of the catalogs based on the metrics’ categorization.	Text classification; High dimensionality; Feature selection; Filtering method; Variable Ranking; ALOFTSoftware maintainability; Metrics; Object-Oriented Software Development
Variant-rich software systems offer a large degree of customization, allowing users to configure the target system according to their preferences and needs. Facing high degrees of variability, these systems often employ variability models to explicitly capture user-configurable features (e.g., systems options) and the constraints they impose. The explicit representation of features allows them to be referenced in different variation points across different artifacts, enabling the latter to vary according to specific feature selections. In such settings, the evolution of variability models interplays with the evolution of related artifacts, requiring the two to evolve together, or coevolve. Interestingly, little is known about how such coevolution occurs in real-world systems, as existing research has focused mostly on variability evolution as it happens in variability models only. Furthermore, existing techniques supporting variability evolution are usually validated with randomly-generated variability models or evolution scenarios that do not stem from practice. As the community lacks a deep understanding of how variability evolution occurs in real-world systems and how it relates to the evolution of different kinds of software artifacts, it is not surprising that industry reports existing tools and solutions ineffective, as they do not handle the complexity found in practice. Attempting to mitigate this overall lack of knowledge and to support tool builders with insights on how variability models coevolve with other artifact types, we study a large and complex real-world variant-rich software system: the Linux kernel. Specifically, we extract variability-coevolution patterns capturing changes in the variability model of the Linux kernel with subsequent changes in Makefiles and C source code. From the analysis of the patterns, we report on findings concerning evolution principles found in the kernel, and we reveal deficiencies in existing tools and theory when handling changes captured by our patterns.	Variability Evolution Software product lines Patterns Linux
The abnormal deposition of amyloid-β protein in the brain plays an important role in Alzheimer's disease (AD), being considered a potential clinical biomarker. To investigate genetic associations with amyloid-β we used biomarker data and genome-wide variants from individuals with AD and mild cognitive impairment in the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. We used a standard linear model and retested the associations with a mixed linear model to correct the residual sample structure. Both methods' results showed two identical significant SNPs associated with the A β-42 levels in CSF (rs2075650 at intron region TOMM40 with p-value ≥ 1 × 10-16 and rs439401 in the intergenic region of LOC100129500 and APOC1 with p-value ≥ 1 × 10-9) and highlighted APOC1 and TOMM40, which are well-known genes previously associated with AD. Extending our analysis, we considered possible candidate genes mapped to SNPs with p-value ≥ 1 × 10-6 to explore gene-set enrichment e gene-gene network analysis, which reveals genes related to synaptic transmission, transmission of nerve impulses, cell-cell signaling and neurological processes. These genes require fine mapping and replication studies to allow more detailed understanding of how they may contribute to the genetic architecture of AD.	A β-42; Alzheimer’s Disease Neuroimaging Initiative (ADNI); Alzheimer’s disease; Biomarkers; Mild cognitive impairment
Bag-of-words is the most used representation method in text categorization. It represents each document as a feature vector where each vector position represents a word. Since all words in the database are considered features, the feature vector can reach tens of thousands of features. Therefore, text categorization relies on feature selection to eliminate meaningless data and to reduce the execution time. In this paper, we propose two filtering methods for feature selection in text categorization, namely: Maximum f Features per Document (MFD), and Maximum f Features per Document – Reduced (MFDR). Both algorithms determine the number of selected features f in a data-driven way using a global-ranking Feature Evaluation Function (FEF). The MFD method analyzes all documents to ensure that each document in the training set is represented in the final feature vector. Whereas MFDR analyzes only the documents with high FEF valued features to select less features therefore avoiding unnecessary ones. The experimental study evaluated the effectiveness of the proposed methods on four text categorization databases (20 Newsgroup, Reuters, WebKB and TDT2) and three FEFs using the Naïve Bayes classifier. The proposed methods present better or equivalent results when compared with the ALOFT method, in all cases, and Variable Ranking, in more than 93% of the cases.	Text classification; High dimensionality; Feature selection; Filtering method; Variable Ranking; ALOFT
Efficient techniques for pattern matching are essential in a number of networked systems and services, such as intrusion detection systems, application identification and classification services, and traffic management. Most pattern matching applications describe patterns using regular expression and the support engine is Deterministic Finite Automaton (FA). Previous research studies address either performance or space requirements issues. From the original DFA formalism we design and evaluate optimizations to its representation and operation to meet Deep Packet Inspection (DPI) systems’ requirements for commodity platforms, such as (i) decreasing the original DFA memory consumption (high compression ratio) and (ii) performing pattern matching as fast as the original DFA. Our approach spans from designing the DFA to developing memory layouts to get the most of the underlying architecture. The contributions of this work are threefold: (i) a new and improved finite automaton model, called Ranged Compressed DFA (RCDFA), (ii) three RCDFA optimizations for achieving more compression and matching speed, and (iii) three advanced layouts for implementing the compressed automaton without memory and performance penalties. The experimental evaluation shows that RCDFA compresses DFA up to 99% without imposing additional memory lookups. The proposed advanced layouts reach memory compression of around 97%. RCDFA together with the advanced layouts outperforms the standard DFA by up to 32 times in terms of processing speed.	DFA optimization; Deep Packet Inspection; Performance evaluation; Computer network
It appears uncontroversial that digital consumers deserve the same level of protection for their transactions as that offered in other forms of commerce. The question is whether current consumer protection instruments provide these protections. Courts in Europe have adopted a view that, by analogy, one should grant 'product-protection' to digital content contracts on a case-by-case basis. Unfortunately, this means that the digital content category remains highly controversial and uncertain to the stakeholders of such transactions. On the other hand, the Brazilian Consumer Protection Law (CDC) has adopted more innovative approaches to consumer rights than many European Union (EU) Member States' national legislation, both when it has a broad understanding of 'goods' and 'services' and when it provides just minor differences between the legal remedies and rights for the sale of goods and service contracts. Here, we argue that there is no reason to craft a third sui generis category for digital content per se, and to the cloud-based transactions, for the supply of digital content in Brazil, if the current legal framework in the Brazilian CDC in relation to these issues is interpreted extensively. We further claim that this remains the best approach to be adopted.	cloud law, digital content, consumers rights, cloud computing, cloud contract
This article proposes a new data collision free medium access control protocol for wireless sensor networks in automation environments based on time division multiple access (TDMA) with duty cycling, which presents less energy consumption than the S-MAC protocol while maintain high data throughput. The new protocol employs single channel and a carrier sense approach. Simulation results are presented and show how the new proposal outperforms the S-MAC protocol.	Collision free, Energy Efficiency, MAC Protocol
Modularity benefits, including the independent maintenance and comprehension of individual modules, have been widely advocated. However, empirical assessments to investigate those benefits have mostly focused on source code, and thus, the relevance of modularity to earlier artifacts is still not so clear (such as requirements and design models). In this paper, we use a multimethod technique, including designed experiments, to empirically evaluate the benefits of modularity in the context of two approaches for specifying product line use case scenarios: PLUSS and MSVCM. The first uses an annotative approach for specifying variability, whereas the second relies on aspect-oriented constructs for separating common and variant scenario specifications. After evaluating these approaches through the specifications of several systems, we find out that MSVCM reduces feature scattering and improves scenario cohesion. These results suggest that evolving a product line specification using MSVCM requires only localized changes. On the other hand, the results of six experiments reveal that MSVCM requires more time to derive the product line specifications and, contrasting with the modularity results, reduces the time to evolve a product line specification only when the subjects have been well trained and are used to the task of evolving product line specifications.	Usage scenarios Requirements engineering Software modularity Software product lines Experimentation in software engineering
Dendritic cells (DC) are professional antigen presenting cells that develop from hematopoietic stem cells through successive steps of lineage commitment and differentiation. Multipotent progenitors (MPP) are committed to DC restricted common DC progenitors (CDP), which differentiate into specific DC subsets, classical DC (cDC) and plasmacytoid DC (pDC). To determine epigenetic states and regulatory circuitries during DC differentiation, we measured consecutive changes of genome-wide gene expression, histone modification and transcription factor occupancy during the sequel MPP-CDP-cDC/pDC. Specific histone marks in CDP reveal a DC-primed epigenetic signature, which is maintained and reinforced during DC differentiation. Epigenetic marks and transcription factor PU.1 occupancy increasingly coincide upon DC differentiation. By integrating PU.1 occupancy and gene expression we devised a transcription factor regulatory circuitry for DC commitment and subset specification. The circuitry provides the transcription factor hierarchy that drives the sequel MPP-CDP-cDC/pDC, including Irf4, Irf8, Tcf4, Spib and Stat factors. The circuitry also includes feedback loops inferred for individual or multiple factors, which stabilize distinct stages of DC development and DC subsets. In summary, here we describe the basic regulatory circuitry of transcription factors that drives DC development.	Animals Cell Lineage Cells, Cultured Dendritic Cells/metabolism* Epigenesis, Genetic* Gene Regulatory Networks* Hematopoietic Stem Cells/metabolism Histones/metabolism Mice Proto-Oncogene Proteins/metabolism Trans-Activators/metabolism Transcription Factors/metabolism*
In time series forecasting exercises it has been usual to suppose that the error series generated by the forecasters have a white noise behavior. However, it is possible that such supposition is violated in practice due to model misspecification or disturbances of the phenomenon not captured by the predictive models. It may lead to statistically biased and/or inefficient predictors. The present paper introduces an approach to correct predetermined forecasters by recursively modeling their remaining residuals. Two formalisms are used to illustrate the recursive approach: the well-known (linear) autoregressive integrated moving average (ARIMA) and the (non-linear) Artificial Neural Network (ANN). These models are recursively adjusted to the remaining residuals of a given forecaster until a white noise behavior is achieved. Applications involving ARIMA and ANN forecasters for Dow Jones Industrial Average Index, S&P500 Index, Google Stock Value, Nasdaq Index, Wolf׳s Sunspot, and Canadian Lynx data series indicate the usefulness of the proposed framework.	Time series forecasters; Residuals series; ARIMA; Artificial neural networks
High availability in cloud computing services is essential for maintaining customer confidence and avoiding revenue losses due to SLA violation penalties. Since the software and hardware components of cloud infrastructures may have limited reliability, the use of redundant components and multiple clusters may be required to achieve the expected level of dependability while also increasing the computational capacity. A drawback of such improvements is the respective impact on the capital and increase in acquisition and operational costs. This paper presents availability models for private cloud architectures based on Eucalyptus platform, and presents a comparison of costs between these architectures and similar infrastructure rented from a public cloud provider. Metrics for capacity-oriented availability and system steady-state availability are used to compare architectures with distinct numbers of clusters. A heterogeneous hierarchical modeling approach is employed to represent the systems considering both hardware and software failures. The results highlight that improvements on the availability are not significant when increasing the system to more than two clusters. The analysis also shows that the average available capacity is close to the maximum possible capacity in all architectures, and that it takes 18 months, in average, for these private cloud architectures to pay off the cost equivalent to the computational capacity rented from a public cloud.	Cloud computing Availability Capacity oriented availability Analytical models
Evolutionary algorithms (EAs) can be used to find solutions in dynamic environments. In such cases, after a change in the environment, EAs can either be restarted or they can take advantage of previous knowledge to resume the evolutionary process. The second option tends to be faster and demands less computational effort. The preservation or growth of population diversity is one of the strategies used to advance the evolutionary process after modifications to the environment. We propose a new adaptive method to control population diversity based on a model-reference. The EA evolves the population whereas a control strategy, independently, handles the population diversity. Thus, the adaptive EA evolves a population that follows a diversity-reference model. The proposed model, called the Diversity-Reference Adaptive Control Evolutionary Algorithm (DRAC), aims to maintain or increase the population diversity, thus avoiding premature convergence, and assuring exploration of the solution space during the whole evolutionary process. We also propose a diversity models based on the dynamics of heterozygosity of the population, as models to be tracked by the diversity control. The performance of DRAC showed promising results when compared with the standard genetic algorithm and six other adaptive evolutionary algorithms in 14 different experiments with three different types of environments.	Evolutionary algorithms; adaptive control; dynamic environment; parameter control; diversity
In this article, we propose a numerical approach to the far field reflector problem which is an inverse problem arising in geometric optics. Caffarelli et al. (Contemp Math 226:13–32, 1999) proposed an algorithm that involves the computation of the intersection of the convex hull of confocal paraboloids. We show that computing this intersection amounts to computing the intersection of a power diagram (a generalization of the Voronoi diagram) with the unit sphere. This allows us to provide an algorithm that computes efficiently the intersection of confocal paraboloids using the exact geometric computation paradigm. Furthermore, using an optimal transport formulation, we cast the far field reflector problem into a concave maximization problem. This allows us to numerically solve the far field reflector problem with up to 15k paraboloids. We also investigate other geometric optic problems that involve union of confocal paraboloids and also intersection and union of confocal ellipsoids. In all these cases, we show that the computation of these surfaces is equivalent to the computation of the intersection of a power diagram with the unit sphere.	68U05 Computer graphics; computational geometry 65K15 Mathematical programming, optimization and variational techniques 52A41 Convex functions and convex programs 35Q99 Equations of mathematical physics and other areas of application
In this article, we present a systematic mapping study of research on personality in software engineering. The goal is to plot the landscape of current published empirical and theoretical studies that deal with the role of personality in software engineering. We applied the systematic review method to search and select published articles, and to extract and synthesize data from the selected articles that reported studies about personality. Our search retrieved more than 19,000 articles, from which we selected 90 articles published between 1970 and 2010. Nearly 72% of the studies were published after 2002 and 83% of the studies reported empirical research findings. Data extracted from the 90 studies showed that education and pair programming were the most recurring research topics, and that MBTI was the most used test. Research related to pair programming, education, team effectiveness, software process allocation, software engineer personality characteristics, and individual performance concentrated over 88% of the studies, while team process, behavior and preferences, and leadership performance were the topics with the smallest number of studies. We conclude that the number of articles has grown in the last few years, but contradictory evidence was found that might have been caused by differences in context, research method, and versions of the tests used in the studies. While this raises a warning for practitioners that wish to use personality tests in practice, it shows several opportunities for the research community to improve and extend findings in this field.	Human factors in software engineering; Software psychology; Empirical software engineering; Mapping study; Systematic literature review
This paper has two basic objectives: the first is to investigate Hopf Bifurcation in the internal state of a Chaotic Associative Memory (CAM). For a small network with three neurons, resulting in a six-dimensional Equation of State, the existence and stability of Hopf Bifurcation were verified analytically. The second objective is to study how the Hopf Bifurcation changed the external state (output) of CAM, since this network was trained to associate a dataset of input–output patterns. There were three main differences between this study and others: the bifurcation parameter was not a time delay, but a physical parameter of a CAM; the weights of interconnections between chaotic neurons were neither free parameters nor chosen arbitrarily, but determined in the training process of classical AM; the Hopf Bifurcation occurred in the internal state of CAM, and not in the external state (input–output network signal). We present three examples of Hopf Bifurcation: one neuron with supercritical bifurcation while the other two neurons do not bifurcate; two neurons bifurcating into a subcritical bifurcation and one neuron does not bifurcate; and the same example as before, but with a supercritical bifurcation. We show that the presence of a limit cycle in the internal state of CAM prevents output signals from the network converging towards a desired equilibrium state (desired memory), although the CAM is able to access this memory.	Chaotic Neural Network; Hopf Bifurcation; Associative Memory
Fuzzy clustering has become an important research field with many applications to real world problems. Among fuzzy clustering methods, fuzzy c-means (FCM) is one of the best known for its simplicity and efficiency, although it shows some weaknesses, particularly its tendency to fall into local minima. To tackle this shortcoming, many optimization-based fuzzy clustering methods have been proposed in the literature. Some of these methods are based solely on a metaheuristic optimization, such as particle swarm optimization (PSO) whereas others are hybrid methods that combine a metaheuristic with a traditional partitional clustering method such as FCM. It is demonstrated in the literature that methods that hybridize PSO and FCM for clustering have an improved accuracy over traditional partitional clustering approaches. On the other hand, PSO-based clustering methods have poor execution time in comparison to partitional clustering techniques. Another problem with PSO-based clustering is that the current PSO algorithms require tuning a range of parameters before they are able to find good solutions. In this paper we introduce two hybrid methods for fuzzy clustering that aim to deal with these shortcomings. The methods, referred to as FCM–IDPSO and FCM2–IDPSO, combine FCM with a recent version of PSO, the IDPSO, which adjusts PSO parameters dynamically during execution, aiming to provide better balance between exploration and exploitation, avoiding falling into local minima quickly and thereby obtaining better solutions. Experiments using two synthetic data sets and eight real-world data sets are reported and discussed. The experiments considered the proposed methods as well as some recent PSO-based fuzzy clustering methods. The results show that the methods introduced in this paper provide comparable or in many cases better solutions than the other methods considered in the comparison and were much faster than the other state of the art PSO-based methods.	Fuzzy clustering; Fuzzy c-means; Improved particle swarm optimization; Adaptive weights
Replications; Experiments; Empirical studies; Mapping study; Systematic literature review; Software engineeringSeveral missing value imputation methods for gene expression data have been proposed in the literature. In the past few years, researchers have been putting a great deal of effort into presenting systematic evaluations of the different imputation algorithms. Initially, most algorithms were assessed with an emphasis on the accuracy of the imputation, using metrics such as the root mean squared error. However, it has become clear that the success of the estimation of the expression value should be evaluated in more practical terms as well. One can consider, for example, the ability of the method to preserve the significant genes in the dataset, or its discriminative/predictive power for classification/clustering purposes. Results and conclusions We performed a broad analysis of the impact of five well-known missing value imputation methods on three clustering and four classification methods, in the context of 12 cancer gene expression datasets. We employed a statistical framework, for the first time in this field, to assess whether different imputation methods improve the performance of the clustering/classification methods. Our results suggest that the imputation methods evaluated have a minor impact on the classification and downstream clustering analyses. Simple methods such as replacing the missing values by mean or the median values performed as well as more complex strategies. The datasets analyzed in this study are available at http://costalab.org/Imputation/.	Missing data Imputation Clustering Classification Gene expression
Smart cities have become a reality around the world. They rely on wireless communication technologies, and they have provided many benefits to society, such as monitoring road traffic in real-time, giving continuous healthcare assistance to residents and managing the environment. This article revisits key interoperability questions in heterogeneous wireless networks for smart cities, and outlines a simple, modular architecture to deal with these complex issues. The architecture is composed by sensing, access network, Internet/cloud and application layers. Different features provided by the architecture, such as interoperability among technologies, low cost, reliability and security, have been evaluated through experiments and simulations under different scenarios. The QoS support and the seamless connectivity between pairs of heterogeneous technologies are proposed through a policy-based management (PBM) framework and MIH (Media Independent Handover). Moreover, an 802.11 mesh backbone composed of different types of mesh routers has been deployed for interconnecting the sensors and actuators to the Internet. Key results from experiments in the backbone are examined. They compare: (i) the performance of a single-path routing protocol (OLSR) with a multipath one (MP-OLSR); (ii) the monitoring delays from the proposed low cost sunspot/mesh and arduino/mesh gateways; and (iii) the authentication mechanisms employed. Significant results from simulations allow the analysis of the reliability on vehicular/mesh networks under jamming attacks by applying the OLSR and MP-OLSR routing protocols. Finally, this article provides an overview of open research questions.	Smart cities; Heterogeneous wireless communication; Architecture; Low cost; Interoperability
Context Two recent mapping studies which were intended to verify the current state of replication of empirical studies in Software Engineering (SE) identified two sets of studies: empirical studies actually reporting replications (published between 1994 and 2012) and a second group of studies that are concerned with definitions, classifications, processes, guidelines, and other research topics or themes about replication work in empirical software engineering research (published between 1996 and 2012). Objective In this current article, our goal is to analyze and discuss the contents of the second set of studies about replications to increase our understanding of the current state of the work on replication in empirical software engineering research. Method We applied the systematic literature review method to build a systematic mapping study, in which the primary studies were collected by two previous mapping studies covering the period 1996–2012 complemented by manual and automatic search procedures that collected articles published in 2013. Results We analyzed 37 papers reporting studies about replication published in the last 17 years. These papers explore different topics related to concepts and classifications, presented guidelines, and discuss theoretical issues that are relevant for our understanding of replication in our field. We also investigated how these 37 papers have been cited in the 135 replication papers published between 1994 and 2012. Conclusions Replication in SE still lacks a set of standardized concepts and terminology, which has a negative impact on the replication work in our field. To improve this situation, it is important that the SE research community engage on an effort to create and evaluate taxonomy, frameworks, guidelines, and methodologies to fully support the development of replications.	Replications; Experiments; Empirical studies; Mapping study; Systematic literature review; Software engineering
Open Government Data OGD hold great promise for transforming the efficiency and effectiveness of public services through the ease of publishing and access to government public information or through the offer of new kinds of services, such as smart cities services and applications. In this work, we analyze the Brazilian OGD current scenario and the main difficulties and challenges of developing applications using that data. First, we performed a structured analysis of Brazilian OGD repositories according to OGD definitions. Then, we analyzed the development of two similar applications that use the OGD of two main Brazilian cities and were submitted to different cities' application contests and were well evaluated in both of them. Based on the analysis, this work concludes that Brazilian OGD initiatives have to resolve some issues before being considered truly open data for use in application development at large.	Brazil, Digital Society, E-Government, Open Cities, Open  Data, Open Government Data,
In this paper, a new sequential learning algorithm is constructed by combining the Online Sequential Extreme Learning Machine (OS-ELM) and Kalman filter regression. The Kalman Online Sequential Extreme Learning Machine (KOSELM) handles the problem of multicollinearity of the OS-ELM, which can generate poor predictions and unstable models. The KOSELM learns the training data one-by-one or chunk-by-chunk by adjusting the variance of the output weights through the Kalman filter. The performance of the proposed algorithm has been validated on benchmark regression datasets, and the results show that KOSELM can achieve a higher learning accuracy than OS-ELM and its related extensions. A statistical validation for the differences of the accuracy for all algorithms is performed, and the results confirm that KOSELM has better stability than ReOS-ELM, TOSELM and LS-IELM.	Online sequential learning; Extreme learning machine; Online Sequential Extreme Learning Machine; Kalman filter regression; Multicollinearity
As atuais perspectivas computacionais, vindas sobretudo da Web, têm gerado novas demandas relacionadas ao gerenciamento de dados, principalmente em termos de volume, heterogeneidade e dinamismo. Uma tendência atual para facilitar o gerenciamento de dados na Web é a utilização dos denominados Sistemas NoSQL, que se diferenciam dos sistemas que seguem o Modelo Relacional por possibilitarem a implementação de estruturas mais flexíveis. Contudo, a maioria dos bancos de dados de aplicações existentes encontra-se em estruturas relacionais, e a migração de uma base que segue o Modelo Relacional para uma NoSQL requer grande esforço dos projetistas diante das diferenças existentes. Nesse panorama, este artigo descreve os modelos citados, em termos de conceitos e estruturas, e apresenta um estudo comparativo apontando possíveis mapeamentos conceituais entre eles. Aborda também, de forma comparativa, trabalhos de conversão de dados existentes, e indica desafios e possibilidades para novas pesquisas sobre o tema.	Modelo relacional. NoSQL. Mapeamento conceitual. Conversão de dados.
Dynamic ensemble selection systems work by estimating the level of competence of each classifier from a pool of classifiers. Only the most competent ones are selected to classify a given test sample. This is achieved by defining a criterion to measure the level of competence of a base classifier, such as, its accuracy in local regions of the feature space around the query instance. However, using only one criterion about the behavior of a base classifier is not sufficient to accurately estimate its level of competence. In this paper, we present a novel dynamic ensemble selection framework using meta-learning. We propose five distinct sets of meta-features, each one corresponding to a different criterion to measure the level of competence of a classifier for the classification of input samples. The meta-features are extracted from the training data and used to train a meta-classifier to predict whether or not a base classifier is competent enough to classify an input instance. During the generalization phase, the meta-features are extracted from the query instance and passed down as input to the meta-classifier. The meta-classifier estimates, whether a base classifier is competent enough to be added to the ensemble. Experiments are conducted over several small sample size classification problems, i.e., problems with a high degree of uncertainty due to the lack of training data. Experimental results show that the proposed meta-learning framework greatly improves classification accuracy when compared against current state-of-the-art dynamic ensemble selection techniques.	Ensemble of classifiers; Dynamic ensemble selection; Meta-learning; Classifier competence
A model checker is an automatic tool that traverses a specific structure (normally a Kripke structure referred as the model M) to check the satisfaction of some (temporal) logical property f. This is formally stated as M⊨f. For some formal notations, the model M of a specification S (written in a formal language L) can be described as a labelled transition system (LTS). Specifically, it is not clear in general how usual tools such as SPIN, FDR, PAT, etc., create the LTS representation from a given process. Although one expects the coherence of the LTS generation with the semantics of L, it is completely hidden inside the model checker itself. In this paper we show how to create a model checker for L, using a development approach based on its operational semantics. We use a systematic semantics embedding and the formal modeling using logic programming and analysis (FORMULA) framework to this end. We illustrate our strategy considering the formal language COMPASS modelling language (CML)—a new language that was based on CSP, VDM and the refinement calculus proposed for modelling and analysis of systems of systems. As FORMULA is based on satisfiability modulo theories solving, our model checker can handle communications and predicates involving data with infinite domains by building and manipulating a symbolic LTS. This goes beyond the capabilities of traditional CSP model checkers such as FDR and PAT. Moreover, we show how to reduce time and space complexities by simple semantic modifications in the embedding. This allows a more semantics-preserving tuning. Finally, we show a real implementation of our model checker in an integrated development platform for CML and its practical use on an industrial case study.	CML Model checker Analysis FORMULA Operational semantics SMT
 Degradation on characters and digits may lead to breaks in strokes and even to the splitting of the characters or digits into two or more separate components. These degradations may cause reduction in performance of automatic recognizers and several other procedures applied to document images. It is presented herein a method for reconstructing degraded strings of digits. The method emulates physical forces such as inertial and centripetal force to analyze digits strokes and to perform the reconstruction. The type of path to be followed (straight or curve) is defined automatically such that a specific force is activated. It is shown that the method succeeds in reconstructing breaks present in digits and in performing no reconstructions where none is needed, that is, not linking individual digits. Experimental studies on Brazilian bank checks show the efficacy of the method, which has proven to improve the recognition rates of degraded digit strings by 10 percentage points which is a significant value in financial applications as automatic bank check processing.	Digit degradation, digit reconstruction, physical forces, digit recognition
This article presents a new technology mapper, MogaMap2, the second generation of the technology mapper, MogaMap, based on a hybrid approach that use evolutionary algorithm associated with specific heuristics of the problem in order to find better trade-off results among area, performance and power consumption. Different from MogaMap, the new approach includes a deterministic parameter control that, during the process, modifies the mutation probability. In a set of 20 large designs, we find that this adjust of parameter allow to reduce, in average, the LUT count in 2% and the edge count in 4%. In comparison to state-of-the-art technology mapping, our approach is able to reduce the LUT counts in 3% and the edges count in 10%. Placing and routing the resulting netlists leads to an 3% reduction in the complex logic blocks count, a 7% increasing in estimated operation frequency and 8% reduction in energy consumption.	Evolutionary Algorithm, FPGA, Technology Mapping
Background: Interactive systems are being developed with the intention to help in the engagement of patients on various therapies. Amid the recent technological advances, Kinect™ from Microsoft (Redmond, WA) has helped pave the way on how user interaction technology facilitates and complements many clinical applications. In order to examine the actual status of Kinect developments for rehabilitation, this article presents a systematic review of articles that involve interactive, evaluative, and technical advances related to motor rehabilitation. Materials and Methods: Systematic research was performed in the IEEE Xplore and PubMed databases using the key word combination “Kinect AND rehabilitation” with the following inclusion criteria: (1) English language, (2) page number >4, (3) Kinect system for assistive interaction or clinical evaluation, or (4) Kinect system for improvement or evaluation of the sensor tracking or movement recognition. Quality assessment was performed by QualSyst standards. Results: In total, 109 articles were found in the database research, from which 31 were included in the review: 13 were focused on the development of assistive systems for rehabilitation, 3 in evaluation, 3 in the applicability category, 7 on validation of Kinect anatomic and clinical evaluation, and 5 on improvement techniques. Quality analysis of all included articles is also presented with their respective QualSyst checklist scores. Conclusions: Research and development possibilities and future works with the Kinect for rehabilitation application are extensive. Methodological improvements when performing studies on this area need to be further investigated.	
In the fuzzy k-modes clustering, there is just one membership degree of interest by class for each individual which cannot be sufficient to model ambiguity of data precisely. It is known that the essence of a multivariate thinking allows to expose the inherent structure and meaning revealed within a set of variables classified. In this paper, a multivariate approach for membership degrees is presented to better handle ambiguous data that share properties of different clusters. This method is compared with other fuzzy k-modes methods of the literature based on a multivariate internal index that is also proposed in this paper. Synthetic and real categorical data sets are considered in this study.	Fuzzy clustering Unsupervised pattern recognition Multivariate membership degrees Categorical data
Weld discontinuity in steel tubes was investigated and dimensioned in a data analysis sequence. The correlation matrix, cosine distance and hierarchical cluster were applied as multivariate data processing in this analysis. Welded rings of 9236 mm3 were scanned in gamma ray CT in test tubes and compared with steel base and references. The discontinuity volume detected in the welded rings was assessed based on the pixel volume in data sampling. By modeling gamma ray trajectories and rotation angles in CT scanning, a discontinuity of 0.3 mm was determined and a limit detection of 23 mm3 was obtained.	Gamma transmission; Welded ring; Correlation; Hierarchical cluster; Discrete models
Recent decades have witnessed the birth of major applications of wireless communication technology, further supported by the increasing capabilities of portable devices, low cost and ubiquitous presence. Despite radio technology diversity, a great deal of existing research focuses on a single and isolated wireless technology at a time, where homogeneous elements are identified by IP addresses. This work presents a heterogeneous technology routing (HTR) Framework, targeted towards scenarios where the heterogeneity of devices and networking technologies is present. Our contribution is many fold. It consists of a framework, which encompasses a process for bootstrapping networks, a routing protocol capable of dealing with multiple network interfaces, and a tuning with multipath extensions. We evaluate the performance of the bootstrap, routing and multipath mechanisms by way of simulation and an actual testbed implementation. The multipath evaluation simulates HTR networks with WiMAX, 3GPP LTE and Wi-Fi support. Results show that our proposal can effectively improve the data delivery ratio for ad-hoc networks and that it reduces the end-to-end delay without major impact on network energy consumption. As part of HTR tuning, we investigate next the impacts of tuning the HELLO refresh interval timer on route convergence and its subsequent energy consumption reduction during this phase. We also compare our tuned HTR with the widely used optimized link state routing protocol. Results show that varying the HELLO refresh interval can improve the convergence time and reduce the energy consumption without major impact on network behavior. Our proposal also includes a new distributed address allocation algorithm, namely, the dynamic node configuration protocol (DNCP). This paper conducts a comparative analysis between the Prime, Prophet and the DNCP schemes using static and dynamic topologies in terms of network setup time, energy consumption and control message overhead. Results show that the DNCP had a lower battery power consumption and less control message overhead while it slightly suffers with regard to setup.	Mobile ad hoc networks (MANET) Heterogeneous technologies Address distribution
Binarization of images of old documents is considered a challenging task due to the wide diversity of degradation effects that can be found. To deal with this, many algorithms whose performance depends on an appropriate choice of their parameters have been proposed. In this work, it is investigated the application of a racing procedure based on a statistical approach, named I/F-Race, to suggest the parameters for two binarization algorithms reasoned (i) on the perception of objects by distance (POD) and (ii) on the POD combined with a Laplacian energy-based technique. Our experiments show that both algorithms had their performance statistically improved outperforming other recent binarization techniques. The second proposal presented herein ranked first in H-DIBCO (Handwritten Document Image Binarization Contest) 2014.	Parameter tuning; Document image binarization; Racing algorithms
The power of different intelligent techniques, such as K-means, evolutionary algorithms, artificial immune systems, fuzzy clustering algorithms, kernel regularized methods and others, have been demonstrated over the years by their successful use in many types of problems with different degrees of complexity and in different fields of application. However, the use of a machine learning solution involves choosing a particular model and properly tuning a set of parameters that, together, can effectively and efficiently solve the specific problem. Some difficulties arise in this process, such as the existence of several machine learning algorithms, the exponential number of combinations of parameter values, and also the need for a priori knowledge on the problem domain. Moreover, certain complex learning problems cannot be solved by a single intelligent technique and each intelligent technique has particular computation properties that can be complementary. This special issue has a collection of papers on intelligent systems design. The papers have a combination of many different techniques including fuzzy systems, evolutionary computation, artificial immune systems and clustering algorithms. Data clustering is a fundamental conceptual problem in data mining, which faces the challenger to extract relevant information from distributed data with good computational performance and scalability. For dealing with data distributed in separated repositories, Murilo Naldi and Ricardo Campello proposed in a previous work an algorithm based on Evolutionary k-means for distributed data. Two different distribution approaches of the algorithms were presented: the first obtains a model identical to the centralized version of the clustering algorithm (DF-EAC); the second generates and selects clusters for each distributed data subset and combines them afterwards (CDC). In the paper presented in this special issue, a novel comparison among DF-EAC and CDC algorithms using different validity indices was conducted based on two perspectives: the theoretical one, through asymptotic complexity analyses, and the experimental one, through a comparative evaluation of results obtained from a collection of experiments and statistical tests. Additionally, DF-EAC was revisited and two modifications were investigated: The first preserves data privacy among repositories and the second uses an alternative relative validation index to evaluate the resulting clusters. The modified DFEAC variants are also compared in this study. The obtained results allows to conclude that the first approach is robust to different types of data distribution, and the use of privacy-preserving methods proposed in this study did not jeopardize the quality, speed, or amount of data transmitted by the algorithm. However, the algorithm demands considerable transmission speed if compared to the CDC, being recommended for systems with high transmission rates like local networks and high-speed metropolitan networks. The second approach reduces the data amount and the number of transmissions of the algorithm, causing a slight quality loss, and is recommended for systems with low transmission rates. Finally, the transmission costs of the algorithms do not depend on the number of objects in the data set, no matter what scenario is chosen. Thus, these algorithms are scalable regarding the number of data. The second paper, “An immune-inspired algorithm for a scheduling problem with unrelated parallel machines and sequence and machine dependent-setup times for makespan minimization”, deals with a problem that frequently arise in production environments, the job scheduling. The paper proposes an immune algorithm to solve the unrelated parallel machines scheduling problem with sequence and machine dependent setup-times for makespan minimization. The algorithm is based on clonal selection process and its major features are: the initial population is generated through the construction phase of the Greedy Randomized Adaptive Search Procedure (GRASP); an evaluation function that helps the algorithm to escape from local optima; a Variable Neighborhood Descent (VND) local search heuristic; a somatic hyper mutation operator to accelerate the convergence of the algorithm and a re-selector operator, which strategically keeps good quality solutions with a high level of dispersion in the search space. A full comparison between the implemented algorithms including statistical analysis allows concluding that the proposed algorithm enables better results than those reported in recent literature studies. Moreover, the experiments showed the importance of each operator to the overall performance of the proposed algorithm. Furthermore, when all operators were incorporated into the genetic algorithm, its performance increased, but not enough to overcome the results obtained by the clonal algorithm. Kernel methods are among the most flexible and powerful methods to tackle the problem of non-linear regression estimation. In paper 3, Improving the Kernel Regularized Least Squares Method for Small-Sample Regression, the authors Igor Braga and Maria Carolina Monard, decided to approach this problem by means of Kernel Regularizes Least Squares (KRLS), motivated by the good statistical and computational properties of the method. KRLS performance depends on proper selection of both a kernel function and a regularization parameter. The selection of these elements is very frequently conducted in a cross-validation setting. The most interesting property of KRLS is that it is relatively inexpensive when combined with cross-validation, compared to other learning methods. The authors of paper 3 state that, Gaussian RBF kernel has become a very common kernel choice in much of machine learning research because of their universal approximation properties. However, when combined with cross-validation and small training sets, RBF kernels have a great potential for overfitting. Recently, there has been a renewed interest in developing kernels with less potential for overfitting while retaining a good approximation property. Having that in mind, in this paper they investigate the use of splines as a safer choice to compose a multidimensional kernel function and proposed the use of additive spline kernels instead of multiplicative ones, employed in their previous work. They claim to have found experimental evidence that the additive version is more appropriate to regression estimation in small sample situations. In a second line of investigation, they consider alternative parameter selection methods that have been shown to perform well for other regression methods. Their study demonstrated that the parameter selection procedure Finite Prediction Error (FPE) is a competitive alternative to cross-validation when using the additive splines kernel. Cluster analysis is a very active research topic in the machine learning area. Roughly speaking, Clustering methods aim to organize a set of items into clusters such that items within each cluster have a high degree of similarity, whereas items belonging to different clusters have a high degree of dissimilarity. These methods have been widely applied in fields such as taxonomy, image processing, information retrieval, and data mining. There is a huge amount of research work on this topic addressing several different aspects of the whole clustering process. In Paper 4, A Multi-View Relational Fuzzy c-Medoid Vectors Clustering Algorithm, the authors Francisco de Carvalho, Filipe de Melo and Yves Lechevallier explored the aspects of representation and multi-view data. Two usual representations of the objects upon which clustering can be based are feature data and relational data. When each object is described by a vector of quantitative or qualitative values, the set of vectors describing the objects is called a feature data. When each pair of objects is represented by a relationship, then we have relational data. In multi-view data, the objects are represented by several (feature or relational) data matrices. Multi-view data can be found in many domains such as bioinformatics, and marketing. The authors of Paper 4 emphasize that, while several clustering models and algorithms have been proposed aiming to cluster feature data, few clustering models have been proposed for relational data. They have also observed that several applications, such as content-based image retrieval, would be strongly benefited by clustering methods for relational data. In their paper, the authors have proposed a multi-view relational fuzzy c-medoid vectors clustering algorithm that is able to give a fuzzy partition of the objects taking into account simultaneously their relational descriptions given by multiple dissimilarity matrices. The method proposed is a modified and extended version of the algorithms they proposed before, namely a single-view fuzzy c-medoid algorithm and the fuzzy cmedoids clustering algorithms based on multiple dissimilarity matrices. The main idea is to obtain a collaborative role of the different dissimilarity matrices aiming to obtain a final consensus partition. These matrices could have been obtained using different sets of variables. This algorithm is designed to give a fuzzy partition and a vector of medoids for each fuzzy cluster as well as to learn a relevance weight for each dissimilarity matrix by optimizing an adequacy criterion that measures the fitting between the fuzzy clusters and their representatives. These relevance weights change at each iteration of the algorithm and are different from one cluster to another. Moreover, various tools for interpreting the fuzzy partition and fuzzy clusters provided by this algorithm are also presented. Several examples illustrate the performance and usefulness of the proposed algorithm. This special issue includes four papers selected among the best contributions of the 2nd Brazilian Conference on Intelligent Systems (BRACIS 2013), which took place in Fortaleza, Brazil, from October 19 to October 24 2013. BRACIS is the most important scientific event in Brazil in Artificial Intelligence (AI) and Computational Intelligence (CI), which originated from the combination of the Brazilian Symposium on Artificial Intelligence – SBIA and the Brazilian Symposium on Neural Networks – SBRN. BRACIS is sponsored by the Brazilian Computer Society (SBC) and promotes research of international level and scientific exchange among researchers, practitioners, scientists, and engineers in related disciplines, congregated in Brazil. The conference is focused on original work addressing theories, methods and novel applications dealing mainly with the use and analysis of AI and CI techniques. BRACIS is an international conference with an international program committee, which includes well- established researchers from Brazil and overseas. The papers have been written in English and are published by the Conference Publishing Services (CPS), a division of the IEEE Computer Society press. The BRACIS 2013 received 100 submissions from several different countries. Among these submissions, 43 full papers were accepted for oral presentation. All papers were reviewed by, at least, two independent specialized referees. The authors of the papers with the best reviews were invited to submit an extended and updated version for this special issue. The selection process emphasized three main aspects: originality, relevance and technical contribution. The papers selected from BRACIS 2013 are listed in the references [1-4]. The new versions were submitted to a rigorous peer review process conducted by international reviewers. Only the papers recommended by the reviewers were accepted for this special issue. We believe that this issue presents a set of very high quality papers. As a result, this edition will provide the readers a rich material of current research on Intelligent Systems and related issues. We would like to thank all the authors for their effort to submit high quality papers and the referees for their meticulous and useful reviews with relevant comments and suggestions that surely improved the quality of this special issue. We would also like to thank the Neurocomputing Editor-in-Chief Tom Heskes, the Journal Editorial Board and Elsevier for the opportunity and for efficiently handling the publication procedure. Finally, we would further like to acknowledge Jacqueline Zhu, Vera Kamphuis and Chandini Emmanuel for their help and careful edition of this issue.	
Background Primary cells enter replicative senescence after a limited number of cell divisions. This process needs to be considered in cell culture experiments, and it is particularly important for regenerative medicine. Replicative senescence is associated with reproducible changes in DNA methylation (DNAm) at specific sites in the genome. The mechanism that drives senescence-associated DNAm changes remains unknown - it may involve stochastic DNAm drift due to imperfect maintenance of epigenetic marks or it is directly regulated at specific sites in the genome. Results In this study, we analyzed the reorganization of nuclear architecture and DNAm changes during long-term culture of human fibroblasts and mesenchymal stromal cells (MSCs). We demonstrate that telomeres shorten and shift towards the nuclear center at later passages. In addition, DNAm profiles, either analyzed by MethylCap-seq or by 450k IlluminaBeadChip technology, revealed consistent senescence-associated hypermethylation in regions associated with H3K27me3, H3K4me3, and H3K4me1 histone marks, whereas hypomethylation was associated with chromatin containing H3K9me3 and lamina-associated domains (LADs). DNA hypermethylation was significantly enriched in the vicinity of genes that are either up- or downregulated at later passages. Furthermore, specific transcription factor binding motifs (e.g. EGR1, TFAP2A, and ETS1) were significantly enriched in differentially methylated regions and in the promoters of differentially expressed genes. Conclusions Senescence-associated DNA hypermethylation occurs at specific sites in the genome and reflects functional changes in the course of replicative senescence. These results indicate that tightly regulated epigenetic modifications during long-term culture contribute to changes in nuclear organization and gene expression. Electronic supplementary material The online version of this article (doi:10.1186/s13148-015-0057-5) contains supplementary material, which is available to authorized users.	Senescence, Long-term culture, Telomeres, Epigenetic, DNA methylation, Transcription factor binding sites, Lamina, Massively parallel sequencing
Software product lines enable generating related software products from reusable assets. Adopting a product line strategy can bring significant quality and productivity improvements. However, evolving a product line can be risky, since it might impact many products. When introducing new features or improving its design, it is important to make sure that the behavior of existing products is not affected. To ensure that, one usually has to analyze different types of artifacts, an activity that can lead to errors. To address this issue, in this work we discover and analyze concrete evolution scenarios from five different product lines. We discover a total of 13 safe evolution templates, which are generic transformations that developers can apply when evolving compositional and annotative product lines, with the goal of preserving the behavior of existing products. We also evaluate the templates by analyzing the evolution history of these product lines. In this evaluation, we observe that the templates can address the modifications that developers performed in the analyzed scenarios, which corroborates the expressiveness of our template set. We also observe that the templates could also have helped to avoid the errors that we identified during our analysis.	Software product lines; Refinement; Evolution
This paper presents an algorithm, self-organizing map-state trajectory generator (SOM-STG), to plan and control legged robot locomotion. The SOM-STG is based on an SOM with a time-varying structure characterized by constructing autonomously close-state trajectories from an arbitrary number of robot postures. Each trajectory represents a cyclical movement of the limbs of an animal. The SOM-STG was designed to possess important features of a central pattern generator, such as rhythmic pattern generation, synchronization between limbs, and swapping between gaits following a single command. The acquisition of data for SOM-STG is based on learning by demonstration in which the data are obtained from different demonstrator agents. The SOM-STG can construct one or more gaits for a simulated robot with six legs, can control the robot with any of the gaits learned, and can smoothly swap gaits. In addition, SOM-STG can learn to construct a state trajectory form observing an animal in locomotion. In this paper, a dog is the demonstrator agent	Central pattern generator (CPG), legged robots, locomotion, neural networks, self-organized maps.
Wireless sensor networks (WSNs) are made up of nodes with limited resources, such as processing, bandwidth, memory and, most importantly, energy. For this reason, it is essential that WSNs always work to reduce the power consumption as much as possible in order to maximize its lifetime. In this context, this paper presents SITRUS (semantic infrastructure for wireless sensor networks), which aims to reduce the power consumption of WSN nodes using ontologies. SITRUS consists of two major parts: a message-oriented middleware responsible for both an oriented message communication service and a reconfiguration service; and a semantic information processing module whose purpose is to generate a semantic database that provides the basis to decide whether a WSN node needs to be reconfigurated or not. In order to evaluate the proposed solution, we carried out an experimental evaluation to assess the power consumption and memory usage of WSN applications built atop SITRUS.	ontology and semantic web; power consumption; semantic infrastructure; software reconfiguration; wireless sensor networks
Resource scarcity is a major obstacle for many mobile applications, since devices have limited energy power and processing potential. As an example, there are applications that seamlessly augment human cognition and typically require resources that far outstrip mobile hardware’s capabilities, such as language translation, speech recognition, and face recognition. A new trend has been explored to tackle this problem, the use of cloud computing. This study presents SmartRank, a scheduling framework to perform load partitioning and offloading for mobile applications using cloud computing to increase performance in terms of response time. We first explore a benchmarking of face recognition application using mobile cloud and confirm its suitability to be used as case study with SmartRank. We have applied the approach to a face recognition process based on two strategies: cloudlet federation and resource ranking through balanced metrics (level of CPU utilization and round-trip time). Second, using a full factorial experimental design we tuned the SmartRank with the most suitable partitioning decision calibrating scheduling parameters. Nevertheless, SmartRank uses an equation that is extensible to include new parameters and make it applicable to other scenarios.	Mobile cloud computing Offloading Partitioning Performance evaluation
Cloud computing systems handle large volumes of data by using almost unlimited computational resources, while spatial data warehouses (SDWs) are multidimensional databases that store huge volumes of both spatial data and conventional data. Cloud computing environments have been considered adequate to host voluminous databases, process analytical workloads and deliver database as a service, while spatial online analytical processing (spatial OLAP) queries issued over SDWs are intrinsically analytical. However, hosting a SDW in the cloud and processing spatial OLAP queries over such database impose novel obstacles. In this article, we introduce novel concepts as cloud SDW and spatial OLAP as a service, and afterwards detail the design of novel schemas for cloud SDW and spatial OLAP query processing over cloud SDW. Furthermore, we evaluate the performance to process spatial OLAP queries in cloud SDWs using our own query processor aided by a cloud spatial index. Moreover, we describe the cloud spatial bitmap index to improve the performance to process spatial OLAP queries in cloud SDWs, and assess it through an experimental evaluation. Results derived from our experiments revealed that such index was capable to reduce the query response time from 58.20 up to 98.89 %.	Spatial data warehouse Cloud computing SOLAP Indexing
Business Process Management involves theoretical and operational elements from areas such as Production Engineering, Management and Informatics. In previous studies, we identified critical success factors of BPM initiatives in Brazilian Public Organizations through two multiple case studies. In this work, we intend to investigate how to manage these factors. To achieve this goal, we conducted two focus groups. Five professionals group with experience (specialists) in BPM initiatives in the public sector attended the first focus. The second was performed in a public organization that is conducting a three-year old BPM initiative. It was evidenced that many strategies suggested by specialists are being applied by the public organization investigated in the second focus group. In addition, other strategies were cited to manage the FCS. The main contribution of this study is to investigate from a practical perspective the critical success factors for BPM initiatives in public organizations.	
Background The prediction of breast cancer intrinsic subtypes has been introduced as a valuable strategy to determine patient diagnosis and prognosis, and therapy response. The PAM50 method, based on the expression levels of 50 genes, uses a single sample predictor model to assign subtype labels to samples. Intrinsic errors reported within this assay demonstrate the challenge of identifying and understanding the breast cancer groups. In this study, we aim to: a) identify novel biomarkers for subtype individuation by exploring the competence of a newly proposed method named CM1 score, and b) apply an ensemble learning, as opposed to the use of a single classifier, for sample subtype assignment. The overarching objective is to improve class prediction. Methods and Findings The microarray transcriptome data sets used in this study are: the METABRIC breast cancer data recorded for over 2000 patients, and the public integrated source from ROCK database with 1570 samples. We first computed the CM1 score to identify the probes with highly discriminative patterns of expression across samples of each intrinsic subtype. We further assessed the ability of 42 selected probes on assigning correct subtype labels using 24 different classifiers from the Weka software suite. For comparison, the same method was applied on the list of 50 genes from the PAM50 method. Conclusions The CM1 score portrayed 30 novel biomarkers for predicting breast cancer subtypes, with the confirmation of the role of 12 well-established genes. Intrinsic subtypes assigned using the CM1 list and the ensemble of classifiers are more consistent and homogeneous than the original PAM50 labels. The new subtypes show accurate distributions of current clinical markers ER, PR and HER2, and survival curves in the METABRIC and ROCK data sets. Remarkably, the paradoxical attribution of the original labels reinforces the limitations of employing a single sample classifiers to predict breast cancer intrinsic subtypes.	Biomarkers, Tumor Breast Neoplasms/diagnosis Breast Neoplasms/genetics Breast Neoplasms/mortality Cluster Analysis Computational Biology/methods Datasets as Topic Female Gene Expression Profiling Gene Expression Regulation, Neoplastic Genomics/methods Humans Prognosis Reproducibility of Results Transcriptome
The continuously increasing consumption of power to access the Internet has been a major concern for network operators and equipment vendors. Passive optical network (PON) systems are widely seen as the future of broadband access. In 2010, ITU-T standardized a protocol-based PON energy efficiency mechanism that is comprised of two main modes, the doze mode and the cyclic sleep mode, which promise to save significant amounts of energy. However, the use of these two standardized alternative modes requires extra signaling and wastes energy. In this article we present the watchful sleep mode, a new mode that unifies the doze and cyclic sleep modes into a single power management mode. The new mode eliminates the extra control signaling and maximizes the amount of energy saved by keeping only the necessary hardware ON. Recently, the watchful sleep mode has been included in the ITU-T G.984 (G-PON) and ITUT G.987 (XG-PON) recommendations. It is expected to be operated as the only power management mode in future PON systems.	Energy efficiency, Optical network units, Passive optical networks, IEEE 802.16 Standard, Receivers, Power demand
This paper is about a doctoral project which is underdevelopment and also has the objective of conceiving a social mechanism between the educational contexts Learning Management Systems (LMS) and Personal Learning Environments (PLE) with a purpose of allowing teachers and tutors immersed in distance education modality to formatively accompany the learners in formal and informal learning situations. The review of literature evinces the difficulties of the LMS in joining with other educational contexts, as the example of the PLE, mainly according to: Interoperability. Integration of activities, Traceability, Single-sign-on execution and Security. The review also allowed conceiving a conceptual model to this social mechanism. The result leads to a model made of four stages and its conception was originated through the literature which referred to the characteristics and limitations between these contexts	
Qualidade de software é um tema recorrente nas organizações especializadas. No Brasil, a SOFTEX criou o programa MPS.BR, tendo o objetivo impulsionar a qualidade do software brasileiro. Adicionalmente, visando maior agilidade nos processos, muitas organizações estão adotando práticas ágeis para apoiar a implementação deste programa de qualidade. Assim, o presente artigo visa apresentar um framework contendo práticas ágeis oriundas das metodologias Crystal, FDD, TDD e eXtreme Programming para apoio à implementação do processo de Projeto e Construção do Produto constante no programa MPS.BR.	
Background The search for adherence to maturity levels by using lightweight processes that require low levels of effort is regarded as a challenge for software development organizations. Objective This study seeks to evaluate, synthesize, and present results on the use of the Capability Maturity Model Integration (CMMI) in combination with agile software development, and thereafter to give an overview of the topics researched, which includes a discussion of their benefits and limitations, the strength of the findings, and the implications for research and practice. Methods The method applied was a Systematic Literature Review on studies published up to (and including) 2011. Results The search strategy identified 3193 results, of which 81 included studies on the use of CMMI together with agile methodologies. The benefits found were grouped into two main categories: those related to the organization in general and those related to the development process, and were organized into subcategories, according to the area to which they refer. The limitations were also grouped into these categories. Using the criteria defined, the strength of the evidence found was considered low. The implications of the results for research and practice are discussed. Conclusion Agile methodologies can be used by companies to reduce efforts in getting to levels 2 and 3 of CMMI, there even being reports of applying agile practices that led to achieving level 5. However, agile methodologies alone, according to the studies, were not sufficient to obtain a rating at a given level, it being necessary to resort to additional practices to do so.	Software process improvement; CMMI; Agile methodology; Benefits; Limitations; Systematic review